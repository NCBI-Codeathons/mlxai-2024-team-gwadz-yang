{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61042b23-56b4-4ee4-92cf-782ffbb5f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0c1189-e7ee-48cf-88d9-fad0a9cbd614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample data (replace with your actual data)\n",
    "# X_train = np.random.rand(100, 450).astype(np.float32)  # Sample input data\n",
    "# y_train = np.random.rand(100, 60).astype(np.float32)   # Sample output labels\n",
    "\n",
    "# # Convert data to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train)\n",
    "# y_train_tensor = torch.tensor(y_train)\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'input_output_list.pickle'), 'rb') as f:\n",
    "    a = pickle.load(f)\n",
    "\n",
    "X = np.array([d[0] for d in a]).astype(np.float32)\n",
    "y = np.array([d[1] for d in a]).astype(np.float32)\n",
    "\n",
    "# Normalize input features\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_normalized, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "X_val_tensor = torch.tensor(X_val)\n",
    "y_val_tensor = torch.tensor(y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a7395b-b115-47b0-8706-27baa1f2d593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 135362.0938, Val Loss: 135955.9531\n",
      "Epoch [2/500], Train Loss: 135361.2188, Val Loss: 135955.1250\n",
      "Epoch [3/500], Train Loss: 135360.4062, Val Loss: 135954.2812\n",
      "Epoch [4/500], Train Loss: 135359.5781, Val Loss: 135953.4062\n",
      "Epoch [5/500], Train Loss: 135358.7188, Val Loss: 135952.5000\n",
      "Epoch [6/500], Train Loss: 135357.8125, Val Loss: 135951.5000\n",
      "Epoch [7/500], Train Loss: 135356.7969, Val Loss: 135950.3906\n",
      "Epoch [8/500], Train Loss: 135355.7031, Val Loss: 135949.1406\n",
      "Epoch [9/500], Train Loss: 135354.4688, Val Loss: 135947.7344\n",
      "Epoch [10/500], Train Loss: 135353.0312, Val Loss: 135946.0781\n",
      "Epoch [11/500], Train Loss: 135351.3906, Val Loss: 135944.1406\n",
      "Epoch [12/500], Train Loss: 135349.4219, Val Loss: 135941.8281\n",
      "Epoch [13/500], Train Loss: 135347.1250, Val Loss: 135939.0781\n",
      "Epoch [14/500], Train Loss: 135344.3750, Val Loss: 135935.7656\n",
      "Epoch [15/500], Train Loss: 135341.1250, Val Loss: 135931.8594\n",
      "Epoch [16/500], Train Loss: 135337.2969, Val Loss: 135927.2656\n",
      "Epoch [17/500], Train Loss: 135332.7500, Val Loss: 135921.8438\n",
      "Epoch [18/500], Train Loss: 135327.4219, Val Loss: 135915.5312\n",
      "Epoch [19/500], Train Loss: 135321.1562, Val Loss: 135908.1250\n",
      "Epoch [20/500], Train Loss: 135313.8125, Val Loss: 135899.5156\n",
      "Epoch [21/500], Train Loss: 135305.2969, Val Loss: 135889.4688\n",
      "Epoch [22/500], Train Loss: 135295.4062, Val Loss: 135877.7969\n",
      "Epoch [23/500], Train Loss: 135283.8750, Val Loss: 135864.3125\n",
      "Epoch [24/500], Train Loss: 135270.6406, Val Loss: 135848.7344\n",
      "Epoch [25/500], Train Loss: 135255.1875, Val Loss: 135830.8594\n",
      "Epoch [26/500], Train Loss: 135237.3438, Val Loss: 135810.3281\n",
      "Epoch [27/500], Train Loss: 135217.1875, Val Loss: 135786.7656\n",
      "Epoch [28/500], Train Loss: 135193.7812, Val Loss: 135759.8438\n",
      "Epoch [29/500], Train Loss: 135167.2500, Val Loss: 135729.2031\n",
      "Epoch [30/500], Train Loss: 135137.0625, Val Loss: 135694.4219\n",
      "Epoch [31/500], Train Loss: 135102.5938, Val Loss: 135655.0312\n",
      "Epoch [32/500], Train Loss: 135063.7969, Val Loss: 135610.5625\n",
      "Epoch [33/500], Train Loss: 135019.6875, Val Loss: 135560.3906\n",
      "Epoch [34/500], Train Loss: 134970.1250, Val Loss: 135503.9531\n",
      "Epoch [35/500], Train Loss: 134914.5000, Val Loss: 135440.7188\n",
      "Epoch [36/500], Train Loss: 134851.7500, Val Loss: 135370.0000\n",
      "Epoch [37/500], Train Loss: 134781.9219, Val Loss: 135291.1875\n",
      "Epoch [38/500], Train Loss: 134703.3906, Val Loss: 135203.5156\n",
      "Epoch [39/500], Train Loss: 134617.7188, Val Loss: 135106.2969\n",
      "Epoch [40/500], Train Loss: 134520.9688, Val Loss: 134998.7500\n",
      "Epoch [41/500], Train Loss: 134416.0781, Val Loss: 134880.0156\n",
      "Epoch [42/500], Train Loss: 134298.5312, Val Loss: 134749.2969\n",
      "Epoch [43/500], Train Loss: 134168.0156, Val Loss: 134605.7344\n",
      "Epoch [44/500], Train Loss: 134025.9062, Val Loss: 134448.4688\n",
      "Epoch [45/500], Train Loss: 133872.2656, Val Loss: 134276.6406\n",
      "Epoch [46/500], Train Loss: 133701.3281, Val Loss: 134089.3125\n",
      "Epoch [47/500], Train Loss: 133516.2344, Val Loss: 133885.5938\n",
      "Epoch [48/500], Train Loss: 133314.7969, Val Loss: 133664.5938\n",
      "Epoch [49/500], Train Loss: 133099.5469, Val Loss: 133425.5625\n",
      "Epoch [50/500], Train Loss: 132860.6875, Val Loss: 133167.3594\n",
      "Epoch [51/500], Train Loss: 132607.2031, Val Loss: 132889.2969\n",
      "Epoch [52/500], Train Loss: 132332.9062, Val Loss: 132590.8906\n",
      "Epoch [53/500], Train Loss: 132034.6719, Val Loss: 132271.5000\n",
      "Epoch [54/500], Train Loss: 131718.4844, Val Loss: 131930.5469\n",
      "Epoch [55/500], Train Loss: 131382.9375, Val Loss: 131567.6250\n",
      "Epoch [56/500], Train Loss: 131026.9922, Val Loss: 131182.5156\n",
      "Epoch [57/500], Train Loss: 130638.3281, Val Loss: 130774.9375\n",
      "Epoch [58/500], Train Loss: 130231.8281, Val Loss: 130344.8281\n",
      "Epoch [59/500], Train Loss: 129812.6250, Val Loss: 129892.3359\n",
      "Epoch [60/500], Train Loss: 129354.5547, Val Loss: 129417.8047\n",
      "Epoch [61/500], Train Loss: 128877.3203, Val Loss: 128922.0078\n",
      "Epoch [62/500], Train Loss: 128398.1875, Val Loss: 128406.0781\n",
      "Epoch [63/500], Train Loss: 127884.4062, Val Loss: 127871.3984\n",
      "Epoch [64/500], Train Loss: 127354.1406, Val Loss: 127319.7266\n",
      "Epoch [65/500], Train Loss: 126807.7422, Val Loss: 126753.1797\n",
      "Epoch [66/500], Train Loss: 126231.2422, Val Loss: 126174.0703\n",
      "Epoch [67/500], Train Loss: 125661.7031, Val Loss: 125585.1406\n",
      "Epoch [68/500], Train Loss: 125070.8516, Val Loss: 124989.4141\n",
      "Epoch [69/500], Train Loss: 124482.0781, Val Loss: 124390.3359\n",
      "Epoch [70/500], Train Loss: 123874.7266, Val Loss: 123791.5078\n",
      "Epoch [71/500], Train Loss: 123271.8828, Val Loss: 123196.5469\n",
      "Epoch [72/500], Train Loss: 122701.4609, Val Loss: 122608.9531\n",
      "Epoch [73/500], Train Loss: 122100.0781, Val Loss: 122032.3672\n",
      "Epoch [74/500], Train Loss: 121520.3203, Val Loss: 121470.4062\n",
      "Epoch [75/500], Train Loss: 120948.1641, Val Loss: 120926.4062\n",
      "Epoch [76/500], Train Loss: 120411.0156, Val Loss: 120403.5078\n",
      "Epoch [77/500], Train Loss: 119883.1250, Val Loss: 119904.0859\n",
      "Epoch [78/500], Train Loss: 119394.3125, Val Loss: 119430.2812\n",
      "Epoch [79/500], Train Loss: 118914.4922, Val Loss: 118983.8750\n",
      "Epoch [80/500], Train Loss: 118465.5547, Val Loss: 118565.6016\n",
      "Epoch [81/500], Train Loss: 118068.5938, Val Loss: 118175.2266\n",
      "Epoch [82/500], Train Loss: 117685.6484, Val Loss: 117811.3359\n",
      "Epoch [83/500], Train Loss: 117324.9375, Val Loss: 117471.0312\n",
      "Epoch [84/500], Train Loss: 116989.7266, Val Loss: 117150.8438\n",
      "Epoch [85/500], Train Loss: 116642.9688, Val Loss: 116846.6953\n",
      "Epoch [86/500], Train Loss: 116370.5469, Val Loss: 116554.5703\n",
      "Epoch [87/500], Train Loss: 116075.4062, Val Loss: 116270.8750\n",
      "Epoch [88/500], Train Loss: 115849.3906, Val Loss: 115992.7891\n",
      "Epoch [89/500], Train Loss: 115553.1562, Val Loss: 115718.3750\n",
      "Epoch [90/500], Train Loss: 115284.1719, Val Loss: 115446.2266\n",
      "Epoch [91/500], Train Loss: 115019.0781, Val Loss: 115174.5000\n",
      "Epoch [92/500], Train Loss: 114773.9297, Val Loss: 114904.6797\n",
      "Epoch [93/500], Train Loss: 114513.4219, Val Loss: 114638.1719\n",
      "Epoch [94/500], Train Loss: 114239.5312, Val Loss: 114376.7188\n",
      "Epoch [95/500], Train Loss: 114019.8750, Val Loss: 114122.4531\n",
      "Epoch [96/500], Train Loss: 113742.6406, Val Loss: 113877.2891\n",
      "Epoch [97/500], Train Loss: 113529.6016, Val Loss: 113642.1953\n",
      "Epoch [98/500], Train Loss: 113301.9141, Val Loss: 113417.9062\n",
      "Epoch [99/500], Train Loss: 113086.6797, Val Loss: 113205.9297\n",
      "Epoch [100/500], Train Loss: 112892.1875, Val Loss: 113006.4609\n",
      "Epoch [101/500], Train Loss: 112702.8047, Val Loss: 112819.4062\n",
      "Epoch [102/500], Train Loss: 112530.2422, Val Loss: 112644.4062\n",
      "Epoch [103/500], Train Loss: 112371.9531, Val Loss: 112480.5938\n",
      "Epoch [104/500], Train Loss: 112225.0625, Val Loss: 112326.9766\n",
      "Epoch [105/500], Train Loss: 112047.5703, Val Loss: 112182.5469\n",
      "Epoch [106/500], Train Loss: 111939.5000, Val Loss: 112046.3125\n",
      "Epoch [107/500], Train Loss: 111812.7266, Val Loss: 111917.2891\n",
      "Epoch [108/500], Train Loss: 111701.9609, Val Loss: 111794.7578\n",
      "Epoch [109/500], Train Loss: 111569.1406, Val Loss: 111678.0625\n",
      "Epoch [110/500], Train Loss: 111461.5000, Val Loss: 111566.6562\n",
      "Epoch [111/500], Train Loss: 111358.3672, Val Loss: 111460.1953\n",
      "Epoch [112/500], Train Loss: 111240.9922, Val Loss: 111358.3984\n",
      "Epoch [113/500], Train Loss: 111152.8828, Val Loss: 111261.0234\n",
      "Epoch [114/500], Train Loss: 111074.7812, Val Loss: 111167.8203\n",
      "Epoch [115/500], Train Loss: 110973.1953, Val Loss: 111078.5625\n",
      "Epoch [116/500], Train Loss: 110889.2109, Val Loss: 110993.0625\n",
      "Epoch [117/500], Train Loss: 110828.2812, Val Loss: 110911.2500\n",
      "Epoch [118/500], Train Loss: 110752.0859, Val Loss: 110833.0469\n",
      "Epoch [119/500], Train Loss: 110687.0859, Val Loss: 110758.0781\n",
      "Epoch [120/500], Train Loss: 110566.0234, Val Loss: 110686.3828\n",
      "Epoch [121/500], Train Loss: 110528.6797, Val Loss: 110617.6328\n",
      "Epoch [122/500], Train Loss: 110475.7266, Val Loss: 110551.8359\n",
      "Epoch [123/500], Train Loss: 110395.4453, Val Loss: 110488.8906\n",
      "Epoch [124/500], Train Loss: 110337.5859, Val Loss: 110428.6172\n",
      "Epoch [125/500], Train Loss: 110274.0469, Val Loss: 110371.0000\n",
      "Epoch [126/500], Train Loss: 110202.1875, Val Loss: 110315.8281\n",
      "Epoch [127/500], Train Loss: 110149.3828, Val Loss: 110262.9922\n",
      "Epoch [128/500], Train Loss: 110119.3203, Val Loss: 110212.4609\n",
      "Epoch [129/500], Train Loss: 110045.7578, Val Loss: 110164.0469\n",
      "Epoch [130/500], Train Loss: 110017.8906, Val Loss: 110117.6797\n",
      "Epoch [131/500], Train Loss: 109958.0625, Val Loss: 110073.2266\n",
      "Epoch [132/500], Train Loss: 109897.1797, Val Loss: 110030.6094\n",
      "Epoch [133/500], Train Loss: 109875.3750, Val Loss: 109989.7422\n",
      "Epoch [134/500], Train Loss: 109845.4688, Val Loss: 109950.5781\n",
      "Epoch [135/500], Train Loss: 109780.0703, Val Loss: 109912.9844\n",
      "Epoch [136/500], Train Loss: 109734.2266, Val Loss: 109876.8203\n",
      "Epoch [137/500], Train Loss: 109736.8906, Val Loss: 109842.0781\n",
      "Epoch [138/500], Train Loss: 109652.4141, Val Loss: 109808.6172\n",
      "Epoch [139/500], Train Loss: 109625.7109, Val Loss: 109776.3750\n",
      "Epoch [140/500], Train Loss: 109606.4375, Val Loss: 109745.2031\n",
      "Epoch [141/500], Train Loss: 109548.8984, Val Loss: 109715.0703\n",
      "Epoch [142/500], Train Loss: 109530.4766, Val Loss: 109685.8594\n",
      "Epoch [143/500], Train Loss: 109515.4766, Val Loss: 109657.5000\n",
      "Epoch [144/500], Train Loss: 109450.1875, Val Loss: 109629.9219\n",
      "Epoch [145/500], Train Loss: 109434.0469, Val Loss: 109603.1250\n",
      "Epoch [146/500], Train Loss: 109404.6406, Val Loss: 109576.9922\n",
      "Epoch [147/500], Train Loss: 109388.7109, Val Loss: 109551.4922\n",
      "Epoch [148/500], Train Loss: 109290.7734, Val Loss: 109526.5703\n",
      "Epoch [149/500], Train Loss: 109345.1562, Val Loss: 109502.2266\n",
      "Epoch [150/500], Train Loss: 109303.9297, Val Loss: 109478.5000\n",
      "Epoch [151/500], Train Loss: 109264.9375, Val Loss: 109455.3125\n",
      "Epoch [152/500], Train Loss: 109246.4219, Val Loss: 109432.7031\n",
      "Epoch [153/500], Train Loss: 109170.0234, Val Loss: 109410.6172\n",
      "Epoch [154/500], Train Loss: 109188.5078, Val Loss: 109389.0156\n",
      "Epoch [155/500], Train Loss: 109161.5000, Val Loss: 109367.8594\n",
      "Epoch [156/500], Train Loss: 109147.6875, Val Loss: 109347.2188\n",
      "Epoch [157/500], Train Loss: 109140.7656, Val Loss: 109327.0469\n",
      "Epoch [158/500], Train Loss: 109087.6719, Val Loss: 109307.2969\n",
      "Epoch [159/500], Train Loss: 109095.5625, Val Loss: 109288.0703\n",
      "Epoch [160/500], Train Loss: 109052.6172, Val Loss: 109269.2266\n",
      "Epoch [161/500], Train Loss: 109052.6406, Val Loss: 109250.8438\n",
      "Epoch [162/500], Train Loss: 109009.8281, Val Loss: 109232.8906\n",
      "Epoch [163/500], Train Loss: 109031.6328, Val Loss: 109215.3828\n",
      "Epoch [164/500], Train Loss: 108960.4297, Val Loss: 109198.2969\n",
      "Epoch [165/500], Train Loss: 108933.9531, Val Loss: 109181.5703\n",
      "Epoch [166/500], Train Loss: 108919.1484, Val Loss: 109165.2891\n",
      "Epoch [167/500], Train Loss: 108939.9922, Val Loss: 109149.3672\n",
      "Epoch [168/500], Train Loss: 108918.4141, Val Loss: 109133.7969\n",
      "Epoch [169/500], Train Loss: 108880.5703, Val Loss: 109118.6328\n",
      "Epoch [170/500], Train Loss: 108850.3438, Val Loss: 109103.7891\n",
      "Epoch [171/500], Train Loss: 108839.4219, Val Loss: 109089.3359\n",
      "Epoch [172/500], Train Loss: 108838.6328, Val Loss: 109075.2109\n",
      "Epoch [173/500], Train Loss: 108804.5000, Val Loss: 109061.5156\n",
      "Epoch [174/500], Train Loss: 108809.8750, Val Loss: 109048.1641\n",
      "Epoch [175/500], Train Loss: 108765.6016, Val Loss: 109035.1484\n",
      "Epoch [176/500], Train Loss: 108745.4766, Val Loss: 109022.4609\n",
      "Epoch [177/500], Train Loss: 108729.5703, Val Loss: 109010.0625\n",
      "Epoch [178/500], Train Loss: 108755.3828, Val Loss: 108997.9766\n",
      "Epoch [179/500], Train Loss: 108729.8516, Val Loss: 108986.2812\n",
      "Epoch [180/500], Train Loss: 108710.9922, Val Loss: 108974.9062\n",
      "Epoch [181/500], Train Loss: 108702.4688, Val Loss: 108963.8359\n",
      "Epoch [182/500], Train Loss: 108692.3359, Val Loss: 108953.1406\n",
      "Epoch [183/500], Train Loss: 108640.7656, Val Loss: 108942.7031\n",
      "Epoch [184/500], Train Loss: 108633.1250, Val Loss: 108932.6016\n",
      "Epoch [185/500], Train Loss: 108624.4844, Val Loss: 108922.8359\n",
      "Epoch [186/500], Train Loss: 108611.7734, Val Loss: 108913.3672\n",
      "Epoch [187/500], Train Loss: 108636.2734, Val Loss: 108904.2031\n",
      "Epoch [188/500], Train Loss: 108604.6875, Val Loss: 108895.3125\n",
      "Epoch [189/500], Train Loss: 108588.8672, Val Loss: 108886.6406\n",
      "Epoch [190/500], Train Loss: 108595.8281, Val Loss: 108878.2109\n",
      "Epoch [191/500], Train Loss: 108579.6719, Val Loss: 108870.0312\n",
      "Epoch [192/500], Train Loss: 108596.7578, Val Loss: 108862.0781\n",
      "Epoch [193/500], Train Loss: 108566.8203, Val Loss: 108854.4219\n",
      "Epoch [194/500], Train Loss: 108543.6016, Val Loss: 108847.0469\n",
      "Epoch [195/500], Train Loss: 108510.9375, Val Loss: 108839.8516\n",
      "Epoch [196/500], Train Loss: 108515.0000, Val Loss: 108832.9219\n",
      "Epoch [197/500], Train Loss: 108511.8125, Val Loss: 108826.2734\n",
      "Epoch [198/500], Train Loss: 108532.2891, Val Loss: 108819.8125\n",
      "Epoch [199/500], Train Loss: 108514.9375, Val Loss: 108813.5312\n",
      "Epoch [200/500], Train Loss: 108504.9922, Val Loss: 108807.5234\n",
      "Epoch [201/500], Train Loss: 108497.6172, Val Loss: 108801.7188\n",
      "Epoch [202/500], Train Loss: 108451.3906, Val Loss: 108796.0703\n",
      "Epoch [203/500], Train Loss: 108472.2734, Val Loss: 108790.6250\n",
      "Epoch [204/500], Train Loss: 108459.5781, Val Loss: 108785.3516\n",
      "Epoch [205/500], Train Loss: 108471.2656, Val Loss: 108780.2500\n",
      "Epoch [206/500], Train Loss: 108465.1953, Val Loss: 108775.4141\n",
      "Epoch [207/500], Train Loss: 108466.4922, Val Loss: 108770.6875\n",
      "Epoch [208/500], Train Loss: 108426.5391, Val Loss: 108766.1172\n",
      "Epoch [209/500], Train Loss: 108433.2422, Val Loss: 108761.7578\n",
      "Epoch [210/500], Train Loss: 108419.7422, Val Loss: 108757.5781\n",
      "Epoch [211/500], Train Loss: 108425.4609, Val Loss: 108753.6016\n",
      "Epoch [212/500], Train Loss: 108405.4141, Val Loss: 108749.6328\n",
      "Epoch [213/500], Train Loss: 108407.1484, Val Loss: 108745.8438\n",
      "Epoch [214/500], Train Loss: 108417.1172, Val Loss: 108742.0625\n",
      "Epoch [215/500], Train Loss: 108401.7266, Val Loss: 108738.4531\n",
      "Epoch [216/500], Train Loss: 108389.8281, Val Loss: 108734.9531\n",
      "Epoch [217/500], Train Loss: 108404.6016, Val Loss: 108731.5156\n",
      "Epoch [218/500], Train Loss: 108397.9609, Val Loss: 108728.1875\n",
      "Epoch [219/500], Train Loss: 108340.0859, Val Loss: 108725.0547\n",
      "Epoch [220/500], Train Loss: 108373.3047, Val Loss: 108721.9844\n",
      "Epoch [221/500], Train Loss: 108364.3750, Val Loss: 108719.0312\n",
      "Epoch [222/500], Train Loss: 108391.2578, Val Loss: 108716.3125\n",
      "Epoch [223/500], Train Loss: 108379.2344, Val Loss: 108713.7109\n",
      "Epoch [224/500], Train Loss: 108330.2969, Val Loss: 108711.1953\n",
      "Epoch [225/500], Train Loss: 108328.6562, Val Loss: 108708.8047\n",
      "Epoch [226/500], Train Loss: 108335.0781, Val Loss: 108706.5234\n",
      "Epoch [227/500], Train Loss: 108343.5938, Val Loss: 108704.2344\n",
      "Epoch [228/500], Train Loss: 108343.8516, Val Loss: 108702.0703\n",
      "Epoch [229/500], Train Loss: 108334.1016, Val Loss: 108700.0469\n",
      "Epoch [230/500], Train Loss: 108326.2891, Val Loss: 108698.0469\n",
      "Epoch [231/500], Train Loss: 108376.6875, Val Loss: 108696.1562\n",
      "Epoch [232/500], Train Loss: 108305.2891, Val Loss: 108694.3125\n",
      "Epoch [233/500], Train Loss: 108354.7031, Val Loss: 108692.4922\n",
      "Epoch [234/500], Train Loss: 108316.3047, Val Loss: 108690.7266\n",
      "Epoch [235/500], Train Loss: 108287.3828, Val Loss: 108689.0547\n",
      "Epoch [236/500], Train Loss: 108337.0000, Val Loss: 108687.4297\n",
      "Epoch [237/500], Train Loss: 108349.6719, Val Loss: 108685.9219\n",
      "Epoch [238/500], Train Loss: 108329.4844, Val Loss: 108684.4375\n",
      "Epoch [239/500], Train Loss: 108290.8984, Val Loss: 108683.0234\n",
      "Epoch [240/500], Train Loss: 108321.5703, Val Loss: 108681.6406\n",
      "Epoch [241/500], Train Loss: 108319.4297, Val Loss: 108680.3750\n",
      "Epoch [242/500], Train Loss: 108308.3750, Val Loss: 108679.1953\n",
      "Epoch [243/500], Train Loss: 108289.7812, Val Loss: 108678.0078\n",
      "Epoch [244/500], Train Loss: 108337.1719, Val Loss: 108676.9453\n",
      "Epoch [245/500], Train Loss: 108328.2891, Val Loss: 108676.0000\n",
      "Epoch [246/500], Train Loss: 108318.3516, Val Loss: 108674.9609\n",
      "Epoch [247/500], Train Loss: 108318.4688, Val Loss: 108673.9531\n",
      "Epoch [248/500], Train Loss: 108303.7734, Val Loss: 108673.1016\n",
      "Epoch [249/500], Train Loss: 108330.3359, Val Loss: 108672.2344\n",
      "Epoch [250/500], Train Loss: 108314.0781, Val Loss: 108671.3281\n",
      "Epoch [251/500], Train Loss: 108305.6797, Val Loss: 108670.4453\n",
      "Epoch [252/500], Train Loss: 108288.2734, Val Loss: 108669.5859\n",
      "Epoch [253/500], Train Loss: 108264.0703, Val Loss: 108668.7031\n",
      "Epoch [254/500], Train Loss: 108274.9844, Val Loss: 108667.7500\n",
      "Epoch [255/500], Train Loss: 108299.5547, Val Loss: 108666.9453\n",
      "Epoch [256/500], Train Loss: 108284.2500, Val Loss: 108666.1641\n",
      "Epoch [257/500], Train Loss: 108279.4062, Val Loss: 108665.3438\n",
      "Epoch [258/500], Train Loss: 108273.3672, Val Loss: 108664.6250\n",
      "Epoch [259/500], Train Loss: 108280.8906, Val Loss: 108663.9766\n",
      "Epoch [260/500], Train Loss: 108281.4766, Val Loss: 108663.3359\n",
      "Epoch [261/500], Train Loss: 108277.7656, Val Loss: 108662.8047\n",
      "Epoch [262/500], Train Loss: 108258.4531, Val Loss: 108662.1641\n",
      "Epoch [263/500], Train Loss: 108282.7891, Val Loss: 108661.5938\n",
      "Epoch [264/500], Train Loss: 108257.5859, Val Loss: 108661.0625\n",
      "Epoch [265/500], Train Loss: 108294.4375, Val Loss: 108660.5625\n",
      "Epoch [266/500], Train Loss: 108317.3516, Val Loss: 108660.2109\n",
      "Epoch [267/500], Train Loss: 108306.0156, Val Loss: 108659.9219\n",
      "Epoch [268/500], Train Loss: 108263.3828, Val Loss: 108659.6641\n",
      "Epoch [269/500], Train Loss: 108271.6719, Val Loss: 108659.4453\n",
      "Epoch [270/500], Train Loss: 108296.0000, Val Loss: 108659.1875\n",
      "Epoch [271/500], Train Loss: 108256.5547, Val Loss: 108658.8984\n",
      "Epoch [272/500], Train Loss: 108265.8828, Val Loss: 108658.5938\n",
      "Epoch [273/500], Train Loss: 108308.4453, Val Loss: 108658.1953\n",
      "Epoch [274/500], Train Loss: 108278.3281, Val Loss: 108657.7500\n",
      "Epoch [275/500], Train Loss: 108266.4531, Val Loss: 108657.3125\n",
      "Epoch [276/500], Train Loss: 108285.4297, Val Loss: 108656.8672\n",
      "Epoch [277/500], Train Loss: 108259.9688, Val Loss: 108656.3438\n",
      "Epoch [278/500], Train Loss: 108254.1406, Val Loss: 108655.8438\n",
      "Epoch [279/500], Train Loss: 108298.7969, Val Loss: 108655.4844\n",
      "Epoch [280/500], Train Loss: 108273.7031, Val Loss: 108655.1328\n",
      "Epoch [281/500], Train Loss: 108236.9844, Val Loss: 108654.7344\n",
      "Epoch [282/500], Train Loss: 108258.9453, Val Loss: 108654.3203\n",
      "Epoch [283/500], Train Loss: 108253.8516, Val Loss: 108654.0703\n",
      "Epoch [284/500], Train Loss: 108227.8672, Val Loss: 108653.7266\n",
      "Epoch [285/500], Train Loss: 108291.3203, Val Loss: 108653.5000\n",
      "Epoch [286/500], Train Loss: 108258.5156, Val Loss: 108653.2578\n",
      "Epoch [287/500], Train Loss: 108276.5156, Val Loss: 108653.1016\n",
      "Epoch [288/500], Train Loss: 108269.2969, Val Loss: 108652.9688\n",
      "Epoch [289/500], Train Loss: 108244.0078, Val Loss: 108652.8750\n",
      "Epoch [290/500], Train Loss: 108285.5078, Val Loss: 108652.7578\n",
      "Epoch [291/500], Train Loss: 108231.7578, Val Loss: 108652.6016\n",
      "Epoch [292/500], Train Loss: 108292.1797, Val Loss: 108652.4531\n",
      "Epoch [293/500], Train Loss: 108258.7656, Val Loss: 108652.2656\n",
      "Epoch [294/500], Train Loss: 108251.2734, Val Loss: 108652.0312\n",
      "Epoch [295/500], Train Loss: 108248.4922, Val Loss: 108651.7891\n",
      "Epoch [296/500], Train Loss: 108239.8984, Val Loss: 108651.5312\n",
      "Epoch [297/500], Train Loss: 108232.4453, Val Loss: 108651.2656\n",
      "Epoch [298/500], Train Loss: 108213.6797, Val Loss: 108651.0156\n",
      "Epoch [299/500], Train Loss: 108302.0469, Val Loss: 108650.7891\n",
      "Epoch [300/500], Train Loss: 108245.0469, Val Loss: 108650.6016\n",
      "Epoch [301/500], Train Loss: 108279.5703, Val Loss: 108650.3750\n",
      "Epoch [302/500], Train Loss: 108242.5859, Val Loss: 108650.2891\n",
      "Epoch [303/500], Train Loss: 108270.0781, Val Loss: 108650.1484\n",
      "Epoch [304/500], Train Loss: 108257.5625, Val Loss: 108650.0859\n",
      "Epoch [305/500], Train Loss: 108247.7969, Val Loss: 108649.9844\n",
      "Epoch [306/500], Train Loss: 108241.0781, Val Loss: 108649.8125\n",
      "Epoch [307/500], Train Loss: 108272.8906, Val Loss: 108649.7188\n",
      "Epoch [308/500], Train Loss: 108222.2812, Val Loss: 108649.5625\n",
      "Epoch [309/500], Train Loss: 108282.0234, Val Loss: 108649.4297\n",
      "Epoch [310/500], Train Loss: 108209.5781, Val Loss: 108649.2656\n",
      "Epoch [311/500], Train Loss: 108263.6250, Val Loss: 108649.0781\n",
      "Epoch [312/500], Train Loss: 108275.7578, Val Loss: 108648.9297\n",
      "Epoch [313/500], Train Loss: 108254.7891, Val Loss: 108648.7578\n",
      "Epoch [314/500], Train Loss: 108235.3438, Val Loss: 108648.5547\n",
      "Epoch [315/500], Train Loss: 108249.1328, Val Loss: 108648.3516\n",
      "Epoch [316/500], Train Loss: 108230.0391, Val Loss: 108648.1797\n",
      "Epoch [317/500], Train Loss: 108256.2500, Val Loss: 108648.0469\n",
      "Epoch [318/500], Train Loss: 108195.9844, Val Loss: 108647.8750\n",
      "Epoch [319/500], Train Loss: 108266.8984, Val Loss: 108647.6641\n",
      "Epoch [320/500], Train Loss: 108239.8906, Val Loss: 108647.4062\n",
      "Epoch [321/500], Train Loss: 108244.5156, Val Loss: 108647.1328\n",
      "Epoch [322/500], Train Loss: 108245.2891, Val Loss: 108646.9062\n",
      "Epoch [323/500], Train Loss: 108247.1719, Val Loss: 108646.6719\n",
      "Epoch [324/500], Train Loss: 108235.8359, Val Loss: 108646.4609\n",
      "Epoch [325/500], Train Loss: 108246.2969, Val Loss: 108646.3438\n",
      "Epoch [326/500], Train Loss: 108277.0391, Val Loss: 108646.2656\n",
      "Epoch [327/500], Train Loss: 108224.6875, Val Loss: 108646.2031\n",
      "Epoch [328/500], Train Loss: 108236.0078, Val Loss: 108646.1953\n",
      "Epoch [329/500], Train Loss: 108241.2266, Val Loss: 108646.1641\n",
      "Epoch [330/500], Train Loss: 108225.8750, Val Loss: 108646.0859\n",
      "Epoch [331/500], Train Loss: 108246.7500, Val Loss: 108646.1797\n",
      "Epoch [332/500], Train Loss: 108229.6016, Val Loss: 108646.0781\n",
      "Epoch [333/500], Train Loss: 108247.3750, Val Loss: 108646.0625\n",
      "Epoch [334/500], Train Loss: 108220.9766, Val Loss: 108645.9922\n",
      "Epoch [335/500], Train Loss: 108243.4688, Val Loss: 108645.9375\n",
      "Epoch [336/500], Train Loss: 108210.7734, Val Loss: 108645.8203\n",
      "Epoch [337/500], Train Loss: 108221.6875, Val Loss: 108645.7578\n",
      "Epoch [338/500], Train Loss: 108223.8438, Val Loss: 108645.5625\n",
      "Epoch [339/500], Train Loss: 108210.6719, Val Loss: 108645.3125\n",
      "Epoch [340/500], Train Loss: 108244.5625, Val Loss: 108645.0781\n",
      "Epoch [341/500], Train Loss: 108230.6094, Val Loss: 108644.8750\n",
      "Epoch [342/500], Train Loss: 108218.9062, Val Loss: 108644.6484\n",
      "Epoch [343/500], Train Loss: 108202.3750, Val Loss: 108644.4609\n",
      "Epoch [344/500], Train Loss: 108230.9766, Val Loss: 108644.3516\n",
      "Epoch [345/500], Train Loss: 108226.2578, Val Loss: 108644.2266\n",
      "Epoch [346/500], Train Loss: 108241.0391, Val Loss: 108644.1562\n",
      "Epoch [347/500], Train Loss: 108221.8594, Val Loss: 108644.1172\n",
      "Epoch [348/500], Train Loss: 108208.9844, Val Loss: 108644.0547\n",
      "Epoch [349/500], Train Loss: 108253.7969, Val Loss: 108644.0078\n",
      "Epoch [350/500], Train Loss: 108257.0078, Val Loss: 108643.8906\n",
      "Epoch [351/500], Train Loss: 108244.0000, Val Loss: 108643.7656\n",
      "Epoch [352/500], Train Loss: 108219.8281, Val Loss: 108643.6172\n",
      "Epoch [353/500], Train Loss: 108243.1953, Val Loss: 108643.4688\n",
      "Epoch [354/500], Train Loss: 108249.6250, Val Loss: 108643.3594\n",
      "Epoch [355/500], Train Loss: 108237.6562, Val Loss: 108643.1953\n",
      "Epoch [356/500], Train Loss: 108216.6484, Val Loss: 108643.0078\n",
      "Epoch [357/500], Train Loss: 108225.8906, Val Loss: 108642.7578\n",
      "Epoch [358/500], Train Loss: 108230.1172, Val Loss: 108642.5234\n",
      "Epoch [359/500], Train Loss: 108203.1797, Val Loss: 108642.2656\n",
      "Epoch [360/500], Train Loss: 108208.1953, Val Loss: 108642.0312\n",
      "Epoch [361/500], Train Loss: 108238.2422, Val Loss: 108641.8203\n",
      "Epoch [362/500], Train Loss: 108227.5625, Val Loss: 108641.6250\n",
      "Epoch [363/500], Train Loss: 108226.3281, Val Loss: 108641.4375\n",
      "Epoch [364/500], Train Loss: 108240.3359, Val Loss: 108641.3594\n",
      "Epoch [365/500], Train Loss: 108224.7969, Val Loss: 108641.3516\n",
      "Epoch [366/500], Train Loss: 108247.3516, Val Loss: 108641.3438\n",
      "Epoch [367/500], Train Loss: 108218.2578, Val Loss: 108641.3047\n",
      "Epoch [368/500], Train Loss: 108212.0078, Val Loss: 108641.2656\n",
      "Epoch [369/500], Train Loss: 108231.6641, Val Loss: 108641.1406\n",
      "Epoch [370/500], Train Loss: 108227.6016, Val Loss: 108641.0234\n",
      "Epoch [371/500], Train Loss: 108221.4062, Val Loss: 108640.8203\n",
      "Epoch [372/500], Train Loss: 108229.4844, Val Loss: 108640.5547\n",
      "Epoch [373/500], Train Loss: 108231.5469, Val Loss: 108640.2734\n",
      "Epoch [374/500], Train Loss: 108251.9297, Val Loss: 108640.0078\n",
      "Epoch [375/500], Train Loss: 108249.9688, Val Loss: 108639.7969\n",
      "Epoch [376/500], Train Loss: 108217.4688, Val Loss: 108639.5547\n",
      "Epoch [377/500], Train Loss: 108226.4922, Val Loss: 108639.3047\n",
      "Epoch [378/500], Train Loss: 108199.7422, Val Loss: 108639.0938\n",
      "Epoch [379/500], Train Loss: 108197.0000, Val Loss: 108638.8359\n",
      "Epoch [380/500], Train Loss: 108190.7734, Val Loss: 108638.6016\n",
      "Epoch [381/500], Train Loss: 108243.3672, Val Loss: 108638.4531\n",
      "Epoch [382/500], Train Loss: 108226.3125, Val Loss: 108638.2656\n",
      "Epoch [383/500], Train Loss: 108209.2422, Val Loss: 108638.1250\n",
      "Epoch [384/500], Train Loss: 108220.0859, Val Loss: 108637.9844\n",
      "Epoch [385/500], Train Loss: 108209.8359, Val Loss: 108637.8203\n",
      "Epoch [386/500], Train Loss: 108221.7969, Val Loss: 108637.5312\n",
      "Epoch [387/500], Train Loss: 108211.1406, Val Loss: 108637.2734\n",
      "Epoch [388/500], Train Loss: 108232.8047, Val Loss: 108637.0547\n",
      "Epoch [389/500], Train Loss: 108211.0312, Val Loss: 108636.7969\n",
      "Epoch [390/500], Train Loss: 108213.8516, Val Loss: 108636.5312\n",
      "Epoch [391/500], Train Loss: 108203.7266, Val Loss: 108636.2969\n",
      "Epoch [392/500], Train Loss: 108188.6484, Val Loss: 108636.0312\n",
      "Epoch [393/500], Train Loss: 108214.3906, Val Loss: 108635.7500\n",
      "Epoch [394/500], Train Loss: 108244.4609, Val Loss: 108635.5078\n",
      "Epoch [395/500], Train Loss: 108218.2891, Val Loss: 108635.2969\n",
      "Epoch [396/500], Train Loss: 108161.2188, Val Loss: 108635.0625\n",
      "Epoch [397/500], Train Loss: 108177.3984, Val Loss: 108634.7969\n",
      "Epoch [398/500], Train Loss: 108218.1094, Val Loss: 108634.5391\n",
      "Epoch [399/500], Train Loss: 108208.3594, Val Loss: 108634.2656\n",
      "Epoch [400/500], Train Loss: 108201.1250, Val Loss: 108633.9844\n",
      "Epoch [401/500], Train Loss: 108225.0156, Val Loss: 108633.6641\n",
      "Epoch [402/500], Train Loss: 108211.3906, Val Loss: 108633.4219\n",
      "Epoch [403/500], Train Loss: 108176.5547, Val Loss: 108633.1562\n",
      "Epoch [404/500], Train Loss: 108201.7891, Val Loss: 108632.8125\n",
      "Epoch [405/500], Train Loss: 108177.9141, Val Loss: 108632.5312\n",
      "Epoch [406/500], Train Loss: 108203.2656, Val Loss: 108632.2344\n",
      "Epoch [407/500], Train Loss: 108225.3438, Val Loss: 108631.9219\n",
      "Epoch [408/500], Train Loss: 108209.2344, Val Loss: 108631.6250\n",
      "Epoch [409/500], Train Loss: 108219.8906, Val Loss: 108631.3125\n",
      "Epoch [410/500], Train Loss: 108228.7969, Val Loss: 108631.0938\n",
      "Epoch [411/500], Train Loss: 108223.9141, Val Loss: 108630.9219\n",
      "Epoch [412/500], Train Loss: 108208.5234, Val Loss: 108630.7734\n",
      "Epoch [413/500], Train Loss: 108209.9609, Val Loss: 108630.5938\n",
      "Epoch [414/500], Train Loss: 108175.8750, Val Loss: 108630.3438\n",
      "Epoch [415/500], Train Loss: 108190.5234, Val Loss: 108630.0781\n",
      "Epoch [416/500], Train Loss: 108212.7500, Val Loss: 108629.8125\n",
      "Epoch [417/500], Train Loss: 108171.4375, Val Loss: 108629.5703\n",
      "Epoch [418/500], Train Loss: 108208.1406, Val Loss: 108629.2812\n",
      "Epoch [419/500], Train Loss: 108201.1172, Val Loss: 108628.9297\n",
      "Epoch [420/500], Train Loss: 108194.7500, Val Loss: 108628.6406\n",
      "Epoch [421/500], Train Loss: 108184.8047, Val Loss: 108628.3438\n",
      "Epoch [422/500], Train Loss: 108214.3203, Val Loss: 108628.0156\n",
      "Epoch [423/500], Train Loss: 108218.8359, Val Loss: 108627.6953\n",
      "Epoch [424/500], Train Loss: 108184.8516, Val Loss: 108627.3672\n",
      "Epoch [425/500], Train Loss: 108212.8203, Val Loss: 108627.0625\n",
      "Epoch [426/500], Train Loss: 108207.6094, Val Loss: 108626.7500\n",
      "Epoch [427/500], Train Loss: 108168.3516, Val Loss: 108626.4062\n",
      "Epoch [428/500], Train Loss: 108189.9062, Val Loss: 108626.0078\n",
      "Epoch [429/500], Train Loss: 108181.1797, Val Loss: 108625.6016\n",
      "Epoch [430/500], Train Loss: 108210.8672, Val Loss: 108625.2812\n",
      "Epoch [431/500], Train Loss: 108208.9141, Val Loss: 108624.8906\n",
      "Epoch [432/500], Train Loss: 108208.3828, Val Loss: 108624.6016\n",
      "Epoch [433/500], Train Loss: 108187.1250, Val Loss: 108624.3672\n",
      "Epoch [434/500], Train Loss: 108190.1719, Val Loss: 108623.9219\n",
      "Epoch [435/500], Train Loss: 108184.8438, Val Loss: 108623.4531\n",
      "Epoch [436/500], Train Loss: 108161.6250, Val Loss: 108623.0469\n",
      "Epoch [437/500], Train Loss: 108193.4844, Val Loss: 108622.5391\n",
      "Epoch [438/500], Train Loss: 108173.1250, Val Loss: 108622.0469\n",
      "Epoch [439/500], Train Loss: 108189.2266, Val Loss: 108621.5859\n",
      "Epoch [440/500], Train Loss: 108214.6172, Val Loss: 108621.1094\n",
      "Epoch [441/500], Train Loss: 108258.3281, Val Loss: 108620.6406\n",
      "Epoch [442/500], Train Loss: 108173.5469, Val Loss: 108620.1953\n",
      "Epoch [443/500], Train Loss: 108209.9922, Val Loss: 108619.8125\n",
      "Epoch [444/500], Train Loss: 108195.3672, Val Loss: 108619.3125\n",
      "Epoch [445/500], Train Loss: 108175.5078, Val Loss: 108618.7812\n",
      "Epoch [446/500], Train Loss: 108189.2969, Val Loss: 108618.2266\n",
      "Epoch [447/500], Train Loss: 108184.7812, Val Loss: 108617.6016\n",
      "Epoch [448/500], Train Loss: 108193.8984, Val Loss: 108617.0469\n",
      "Epoch [449/500], Train Loss: 108207.9844, Val Loss: 108616.4453\n",
      "Epoch [450/500], Train Loss: 108234.9141, Val Loss: 108615.9844\n",
      "Epoch [451/500], Train Loss: 108159.3594, Val Loss: 108615.5000\n",
      "Epoch [452/500], Train Loss: 108163.9531, Val Loss: 108614.9609\n",
      "Epoch [453/500], Train Loss: 108182.5312, Val Loss: 108614.4297\n",
      "Epoch [454/500], Train Loss: 108180.7734, Val Loss: 108613.9375\n",
      "Epoch [455/500], Train Loss: 108202.1719, Val Loss: 108613.4141\n",
      "Epoch [456/500], Train Loss: 108221.4609, Val Loss: 108612.9453\n",
      "Epoch [457/500], Train Loss: 108186.4844, Val Loss: 108612.5469\n",
      "Epoch [458/500], Train Loss: 108171.9531, Val Loss: 108612.0859\n",
      "Epoch [459/500], Train Loss: 108182.3281, Val Loss: 108611.5078\n",
      "Epoch [460/500], Train Loss: 108189.6797, Val Loss: 108610.9766\n",
      "Epoch [461/500], Train Loss: 108170.4688, Val Loss: 108610.4062\n",
      "Epoch [462/500], Train Loss: 108149.6172, Val Loss: 108609.7266\n",
      "Epoch [463/500], Train Loss: 108187.9453, Val Loss: 108609.0703\n",
      "Epoch [464/500], Train Loss: 108143.9922, Val Loss: 108608.4922\n",
      "Epoch [465/500], Train Loss: 108176.4141, Val Loss: 108607.9141\n",
      "Epoch [466/500], Train Loss: 108223.4922, Val Loss: 108607.3203\n",
      "Epoch [467/500], Train Loss: 108176.8047, Val Loss: 108606.8281\n",
      "Epoch [468/500], Train Loss: 108157.2422, Val Loss: 108606.2422\n",
      "Epoch [469/500], Train Loss: 108161.6562, Val Loss: 108605.6016\n",
      "Epoch [470/500], Train Loss: 108156.6875, Val Loss: 108604.9844\n",
      "Epoch [471/500], Train Loss: 108179.4609, Val Loss: 108604.3281\n",
      "Epoch [472/500], Train Loss: 108137.4531, Val Loss: 108603.6797\n",
      "Epoch [473/500], Train Loss: 108162.6250, Val Loss: 108602.9453\n",
      "Epoch [474/500], Train Loss: 108145.0938, Val Loss: 108602.1641\n",
      "Epoch [475/500], Train Loss: 108159.1172, Val Loss: 108601.2812\n",
      "Epoch [476/500], Train Loss: 108160.7422, Val Loss: 108600.2500\n",
      "Epoch [477/500], Train Loss: 108179.4141, Val Loss: 108599.2422\n",
      "Epoch [478/500], Train Loss: 108180.1328, Val Loss: 108598.2266\n",
      "Epoch [479/500], Train Loss: 108166.4062, Val Loss: 108597.2500\n",
      "Epoch [480/500], Train Loss: 108129.9375, Val Loss: 108596.3438\n",
      "Epoch [481/500], Train Loss: 108162.2266, Val Loss: 108595.4922\n",
      "Epoch [482/500], Train Loss: 108189.4922, Val Loss: 108594.6641\n",
      "Epoch [483/500], Train Loss: 108168.4453, Val Loss: 108593.9062\n",
      "Epoch [484/500], Train Loss: 108116.4297, Val Loss: 108593.1562\n",
      "Epoch [485/500], Train Loss: 108122.1016, Val Loss: 108592.2812\n",
      "Epoch [486/500], Train Loss: 108142.2344, Val Loss: 108591.4297\n",
      "Epoch [487/500], Train Loss: 108160.9375, Val Loss: 108590.5391\n",
      "Epoch [488/500], Train Loss: 108143.2500, Val Loss: 108589.5781\n",
      "Epoch [489/500], Train Loss: 108110.7734, Val Loss: 108588.6094\n",
      "Epoch [490/500], Train Loss: 108151.2109, Val Loss: 108587.6719\n",
      "Epoch [491/500], Train Loss: 108125.9609, Val Loss: 108586.6719\n",
      "Epoch [492/500], Train Loss: 108130.6953, Val Loss: 108585.6953\n",
      "Epoch [493/500], Train Loss: 108142.9844, Val Loss: 108584.6172\n",
      "Epoch [494/500], Train Loss: 108112.6172, Val Loss: 108583.5625\n",
      "Epoch [495/500], Train Loss: 108170.7656, Val Loss: 108582.5312\n",
      "Epoch [496/500], Train Loss: 108092.2422, Val Loss: 108581.4922\n",
      "Epoch [497/500], Train Loss: 108117.8203, Val Loss: 108580.3281\n",
      "Epoch [498/500], Train Loss: 108126.0469, Val Loss: 108579.1797\n",
      "Epoch [499/500], Train Loss: 108121.0156, Val Loss: 108577.9062\n",
      "Epoch [500/500], Train Loss: 108128.1719, Val Loss: 108576.5781\n",
      "Test Loss: 107514.3047\n",
      "Epoch [1/500], Train Loss: 135367.9375, Val Loss: 135962.2812\n",
      "Epoch [2/500], Train Loss: 135367.3750, Val Loss: 135961.7188\n",
      "Epoch [3/500], Train Loss: 135366.8281, Val Loss: 135961.1562\n",
      "Epoch [4/500], Train Loss: 135366.2500, Val Loss: 135960.5781\n",
      "Epoch [5/500], Train Loss: 135365.6719, Val Loss: 135959.9531\n",
      "Epoch [6/500], Train Loss: 135365.0781, Val Loss: 135959.3438\n",
      "Epoch [7/500], Train Loss: 135364.4219, Val Loss: 135958.6406\n",
      "Epoch [8/500], Train Loss: 135363.7344, Val Loss: 135957.8906\n",
      "Epoch [9/500], Train Loss: 135363.0156, Val Loss: 135957.0938\n",
      "Epoch [10/500], Train Loss: 135362.2031, Val Loss: 135956.2188\n",
      "Epoch [11/500], Train Loss: 135361.3438, Val Loss: 135955.2500\n",
      "Epoch [12/500], Train Loss: 135360.3750, Val Loss: 135954.1875\n",
      "Epoch [13/500], Train Loss: 135359.3125, Val Loss: 135953.0156\n",
      "Epoch [14/500], Train Loss: 135358.1250, Val Loss: 135951.6562\n",
      "Epoch [15/500], Train Loss: 135356.7969, Val Loss: 135950.1406\n",
      "Epoch [16/500], Train Loss: 135355.2656, Val Loss: 135948.4375\n",
      "Epoch [17/500], Train Loss: 135353.5781, Val Loss: 135946.4531\n",
      "Epoch [18/500], Train Loss: 135351.6094, Val Loss: 135944.2031\n",
      "Epoch [19/500], Train Loss: 135349.3594, Val Loss: 135941.6406\n",
      "Epoch [20/500], Train Loss: 135346.7969, Val Loss: 135938.6719\n",
      "Epoch [21/500], Train Loss: 135343.8281, Val Loss: 135935.2500\n",
      "Epoch [22/500], Train Loss: 135340.4531, Val Loss: 135931.3281\n",
      "Epoch [23/500], Train Loss: 135336.5156, Val Loss: 135926.8125\n",
      "Epoch [24/500], Train Loss: 135332.0469, Val Loss: 135921.6406\n",
      "Epoch [25/500], Train Loss: 135326.9375, Val Loss: 135915.7188\n",
      "Epoch [26/500], Train Loss: 135321.0312, Val Loss: 135908.9688\n",
      "Epoch [27/500], Train Loss: 135314.3438, Val Loss: 135901.2500\n",
      "Epoch [28/500], Train Loss: 135306.7188, Val Loss: 135892.4531\n",
      "Epoch [29/500], Train Loss: 135297.9219, Val Loss: 135882.4688\n",
      "Epoch [30/500], Train Loss: 135288.0156, Val Loss: 135871.1094\n",
      "Epoch [31/500], Train Loss: 135276.9062, Val Loss: 135858.2656\n",
      "Epoch [32/500], Train Loss: 135264.2344, Val Loss: 135843.7500\n",
      "Epoch [33/500], Train Loss: 135249.8281, Val Loss: 135827.3594\n",
      "Epoch [34/500], Train Loss: 135233.7344, Val Loss: 135808.9375\n",
      "Epoch [35/500], Train Loss: 135215.0469, Val Loss: 135788.2031\n",
      "Epoch [36/500], Train Loss: 135194.7188, Val Loss: 135765.0156\n",
      "Epoch [37/500], Train Loss: 135172.0000, Val Loss: 135739.0781\n",
      "Epoch [38/500], Train Loss: 135145.9219, Val Loss: 135710.1719\n",
      "Epoch [39/500], Train Loss: 135117.2188, Val Loss: 135678.0156\n",
      "Epoch [40/500], Train Loss: 135085.4844, Val Loss: 135642.2969\n",
      "Epoch [41/500], Train Loss: 135050.0469, Val Loss: 135602.7031\n",
      "Epoch [42/500], Train Loss: 135010.8125, Val Loss: 135558.9219\n",
      "Epoch [43/500], Train Loss: 134967.2656, Val Loss: 135510.6250\n",
      "Epoch [44/500], Train Loss: 134919.4375, Val Loss: 135457.3438\n",
      "Epoch [45/500], Train Loss: 134866.5938, Val Loss: 135398.5781\n",
      "Epoch [46/500], Train Loss: 134808.0625, Val Loss: 135334.0312\n",
      "Epoch [47/500], Train Loss: 134744.0625, Val Loss: 135263.3125\n",
      "Epoch [48/500], Train Loss: 134674.9688, Val Loss: 135186.0000\n",
      "Epoch [49/500], Train Loss: 134597.7500, Val Loss: 135101.7031\n",
      "Epoch [50/500], Train Loss: 134513.9375, Val Loss: 135009.9375\n",
      "Epoch [51/500], Train Loss: 134424.0156, Val Loss: 134910.2500\n",
      "Epoch [52/500], Train Loss: 134323.8750, Val Loss: 134802.1875\n",
      "Epoch [53/500], Train Loss: 134216.3750, Val Loss: 134685.2812\n",
      "Epoch [54/500], Train Loss: 134103.4062, Val Loss: 134559.0156\n",
      "Epoch [55/500], Train Loss: 133977.8438, Val Loss: 134422.9531\n",
      "Epoch [56/500], Train Loss: 133842.4219, Val Loss: 134276.3906\n",
      "Epoch [57/500], Train Loss: 133698.9375, Val Loss: 134118.8906\n",
      "Epoch [58/500], Train Loss: 133543.0625, Val Loss: 133950.0469\n",
      "Epoch [59/500], Train Loss: 133374.4844, Val Loss: 133769.3906\n",
      "Epoch [60/500], Train Loss: 133197.7812, Val Loss: 133576.4375\n",
      "Epoch [61/500], Train Loss: 133003.5000, Val Loss: 133370.8438\n",
      "Epoch [62/500], Train Loss: 132800.6719, Val Loss: 133152.1094\n",
      "Epoch [63/500], Train Loss: 132583.2188, Val Loss: 132919.9219\n",
      "Epoch [64/500], Train Loss: 132354.7812, Val Loss: 132673.9844\n",
      "Epoch [65/500], Train Loss: 132112.4531, Val Loss: 132414.0625\n",
      "Epoch [66/500], Train Loss: 131853.1250, Val Loss: 132139.9531\n",
      "Epoch [67/500], Train Loss: 131584.5156, Val Loss: 131851.5156\n",
      "Epoch [68/500], Train Loss: 131296.2969, Val Loss: 131548.6250\n",
      "Epoch [69/500], Train Loss: 130994.7031, Val Loss: 131231.2500\n",
      "Epoch [70/500], Train Loss: 130678.6875, Val Loss: 130899.3984\n",
      "Epoch [71/500], Train Loss: 130349.7031, Val Loss: 130553.2344\n",
      "Epoch [72/500], Train Loss: 130010.6641, Val Loss: 130192.9688\n",
      "Epoch [73/500], Train Loss: 129651.0156, Val Loss: 129818.9766\n",
      "Epoch [74/500], Train Loss: 129282.2656, Val Loss: 129431.7891\n",
      "Epoch [75/500], Train Loss: 128895.2969, Val Loss: 129032.0703\n",
      "Epoch [76/500], Train Loss: 128500.3125, Val Loss: 128620.6328\n",
      "Epoch [77/500], Train Loss: 128083.8281, Val Loss: 128198.3828\n",
      "Epoch [78/500], Train Loss: 127663.3281, Val Loss: 127766.4453\n",
      "Epoch [79/500], Train Loss: 127229.8203, Val Loss: 127326.1328\n",
      "Epoch [80/500], Train Loss: 126793.3984, Val Loss: 126878.9766\n",
      "Epoch [81/500], Train Loss: 126353.5625, Val Loss: 126426.5547\n",
      "Epoch [82/500], Train Loss: 125898.6406, Val Loss: 125970.5156\n",
      "Epoch [83/500], Train Loss: 125435.6250, Val Loss: 125512.6953\n",
      "Epoch [84/500], Train Loss: 124992.2969, Val Loss: 125055.2109\n",
      "Epoch [85/500], Train Loss: 124523.3359, Val Loss: 124600.0781\n",
      "Epoch [86/500], Train Loss: 124063.0312, Val Loss: 124149.3203\n",
      "Epoch [87/500], Train Loss: 123624.0547, Val Loss: 123704.9922\n",
      "Epoch [88/500], Train Loss: 123171.3125, Val Loss: 123268.9141\n",
      "Epoch [89/500], Train Loss: 122729.6094, Val Loss: 122842.8281\n",
      "Epoch [90/500], Train Loss: 122300.7031, Val Loss: 122428.1016\n",
      "Epoch [91/500], Train Loss: 121893.6094, Val Loss: 122025.7812\n",
      "Epoch [92/500], Train Loss: 121496.8594, Val Loss: 121636.5156\n",
      "Epoch [93/500], Train Loss: 121098.0312, Val Loss: 121260.5547\n",
      "Epoch [94/500], Train Loss: 120716.2344, Val Loss: 120897.9609\n",
      "Epoch [95/500], Train Loss: 120367.8281, Val Loss: 120548.5469\n",
      "Epoch [96/500], Train Loss: 120024.6875, Val Loss: 120211.6484\n",
      "Epoch [97/500], Train Loss: 119679.2109, Val Loss: 119886.3047\n",
      "Epoch [98/500], Train Loss: 119358.0938, Val Loss: 119571.4531\n",
      "Epoch [99/500], Train Loss: 119054.9297, Val Loss: 119266.0078\n",
      "Epoch [100/500], Train Loss: 118748.7812, Val Loss: 118968.8984\n",
      "Epoch [101/500], Train Loss: 118445.7656, Val Loss: 118679.2656\n",
      "Epoch [102/500], Train Loss: 118161.7656, Val Loss: 118395.9688\n",
      "Epoch [103/500], Train Loss: 117872.5859, Val Loss: 118118.3594\n",
      "Epoch [104/500], Train Loss: 117614.5156, Val Loss: 117845.5938\n",
      "Epoch [105/500], Train Loss: 117324.8203, Val Loss: 117577.0469\n",
      "Epoch [106/500], Train Loss: 117072.9453, Val Loss: 117312.6562\n",
      "Epoch [107/500], Train Loss: 116810.7500, Val Loss: 117052.3750\n",
      "Epoch [108/500], Train Loss: 116542.7031, Val Loss: 116796.1562\n",
      "Epoch [109/500], Train Loss: 116350.2500, Val Loss: 116544.0312\n",
      "Epoch [110/500], Train Loss: 116081.6719, Val Loss: 116296.2734\n",
      "Epoch [111/500], Train Loss: 115818.5703, Val Loss: 116053.2266\n",
      "Epoch [112/500], Train Loss: 115598.2812, Val Loss: 115815.4688\n",
      "Epoch [113/500], Train Loss: 115373.0391, Val Loss: 115583.5000\n",
      "Epoch [114/500], Train Loss: 115145.8359, Val Loss: 115357.6016\n",
      "Epoch [115/500], Train Loss: 114918.8984, Val Loss: 115138.1797\n",
      "Epoch [116/500], Train Loss: 114710.3594, Val Loss: 114925.3125\n",
      "Epoch [117/500], Train Loss: 114529.4453, Val Loss: 114719.0312\n",
      "Epoch [118/500], Train Loss: 114330.0000, Val Loss: 114519.2969\n",
      "Epoch [119/500], Train Loss: 114142.3594, Val Loss: 114326.1094\n",
      "Epoch [120/500], Train Loss: 113926.7578, Val Loss: 114139.3125\n",
      "Epoch [121/500], Train Loss: 113758.8984, Val Loss: 113958.8047\n",
      "Epoch [122/500], Train Loss: 113583.6719, Val Loss: 113784.5469\n",
      "Epoch [123/500], Train Loss: 113441.6250, Val Loss: 113616.3125\n",
      "Epoch [124/500], Train Loss: 113274.3047, Val Loss: 113453.9531\n",
      "Epoch [125/500], Train Loss: 113148.3594, Val Loss: 113297.2734\n",
      "Epoch [126/500], Train Loss: 112982.8125, Val Loss: 113146.0156\n",
      "Epoch [127/500], Train Loss: 112823.8594, Val Loss: 113000.0234\n",
      "Epoch [128/500], Train Loss: 112671.5391, Val Loss: 112859.0312\n",
      "Epoch [129/500], Train Loss: 112581.6406, Val Loss: 112722.4297\n",
      "Epoch [130/500], Train Loss: 112440.2344, Val Loss: 112590.1562\n",
      "Epoch [131/500], Train Loss: 112310.2656, Val Loss: 112462.2734\n",
      "Epoch [132/500], Train Loss: 112198.8359, Val Loss: 112338.7656\n",
      "Epoch [133/500], Train Loss: 112056.2891, Val Loss: 112219.3984\n",
      "Epoch [134/500], Train Loss: 111953.2578, Val Loss: 112104.1875\n",
      "Epoch [135/500], Train Loss: 111844.9688, Val Loss: 111992.9688\n",
      "Epoch [136/500], Train Loss: 111746.2188, Val Loss: 111885.6641\n",
      "Epoch [137/500], Train Loss: 111657.6562, Val Loss: 111782.2031\n",
      "Epoch [138/500], Train Loss: 111536.9062, Val Loss: 111682.4609\n",
      "Epoch [139/500], Train Loss: 111463.3047, Val Loss: 111586.3203\n",
      "Epoch [140/500], Train Loss: 111334.2344, Val Loss: 111493.7344\n",
      "Epoch [141/500], Train Loss: 111265.3594, Val Loss: 111404.6484\n",
      "Epoch [142/500], Train Loss: 111196.9375, Val Loss: 111318.9766\n",
      "Epoch [143/500], Train Loss: 111083.3594, Val Loss: 111236.7109\n",
      "Epoch [144/500], Train Loss: 111006.0156, Val Loss: 111157.6328\n",
      "Epoch [145/500], Train Loss: 110931.1250, Val Loss: 111081.7969\n",
      "Epoch [146/500], Train Loss: 110900.9531, Val Loss: 111008.9844\n",
      "Epoch [147/500], Train Loss: 110820.8984, Val Loss: 110939.1875\n",
      "Epoch [148/500], Train Loss: 110754.6641, Val Loss: 110872.1641\n",
      "Epoch [149/500], Train Loss: 110683.5391, Val Loss: 110807.8750\n",
      "Epoch [150/500], Train Loss: 110592.0469, Val Loss: 110746.1328\n",
      "Epoch [151/500], Train Loss: 110557.7812, Val Loss: 110686.8672\n",
      "Epoch [152/500], Train Loss: 110475.4531, Val Loss: 110630.0000\n",
      "Epoch [153/500], Train Loss: 110415.8047, Val Loss: 110575.2422\n",
      "Epoch [154/500], Train Loss: 110400.2266, Val Loss: 110522.6562\n",
      "Epoch [155/500], Train Loss: 110332.8828, Val Loss: 110472.0469\n",
      "Epoch [156/500], Train Loss: 110252.0859, Val Loss: 110423.3047\n",
      "Epoch [157/500], Train Loss: 110217.1641, Val Loss: 110376.3672\n",
      "Epoch [158/500], Train Loss: 110171.8438, Val Loss: 110331.1719\n",
      "Epoch [159/500], Train Loss: 110127.0469, Val Loss: 110287.5547\n",
      "Epoch [160/500], Train Loss: 110078.0938, Val Loss: 110245.4922\n",
      "Epoch [161/500], Train Loss: 110047.1328, Val Loss: 110204.9609\n",
      "Epoch [162/500], Train Loss: 110000.8438, Val Loss: 110165.7344\n",
      "Epoch [163/500], Train Loss: 109976.4219, Val Loss: 110127.8906\n",
      "Epoch [164/500], Train Loss: 109935.7891, Val Loss: 110091.3906\n",
      "Epoch [165/500], Train Loss: 109877.1484, Val Loss: 110056.0000\n",
      "Epoch [166/500], Train Loss: 109841.0781, Val Loss: 110021.8125\n",
      "Epoch [167/500], Train Loss: 109828.1250, Val Loss: 109988.7656\n",
      "Epoch [168/500], Train Loss: 109781.3125, Val Loss: 109956.7188\n",
      "Epoch [169/500], Train Loss: 109762.1797, Val Loss: 109925.6719\n",
      "Epoch [170/500], Train Loss: 109724.0625, Val Loss: 109895.5703\n",
      "Epoch [171/500], Train Loss: 109685.9219, Val Loss: 109866.3438\n",
      "Epoch [172/500], Train Loss: 109684.2109, Val Loss: 109838.0547\n",
      "Epoch [173/500], Train Loss: 109637.5078, Val Loss: 109810.6016\n",
      "Epoch [174/500], Train Loss: 109583.2188, Val Loss: 109783.8750\n",
      "Epoch [175/500], Train Loss: 109545.6484, Val Loss: 109758.0078\n",
      "Epoch [176/500], Train Loss: 109572.2891, Val Loss: 109732.7891\n",
      "Epoch [177/500], Train Loss: 109532.3125, Val Loss: 109708.2500\n",
      "Epoch [178/500], Train Loss: 109510.5938, Val Loss: 109684.4453\n",
      "Epoch [179/500], Train Loss: 109479.1719, Val Loss: 109661.2109\n",
      "Epoch [180/500], Train Loss: 109467.4688, Val Loss: 109638.6094\n",
      "Epoch [181/500], Train Loss: 109408.5547, Val Loss: 109616.5469\n",
      "Epoch [182/500], Train Loss: 109415.5312, Val Loss: 109595.0625\n",
      "Epoch [183/500], Train Loss: 109343.4531, Val Loss: 109574.0469\n",
      "Epoch [184/500], Train Loss: 109357.5000, Val Loss: 109553.5938\n",
      "Epoch [185/500], Train Loss: 109283.5234, Val Loss: 109533.5625\n",
      "Epoch [186/500], Train Loss: 109333.4609, Val Loss: 109514.0703\n",
      "Epoch [187/500], Train Loss: 109277.9141, Val Loss: 109495.0312\n",
      "Epoch [188/500], Train Loss: 109267.5703, Val Loss: 109476.4531\n",
      "Epoch [189/500], Train Loss: 109241.2969, Val Loss: 109458.2734\n",
      "Epoch [190/500], Train Loss: 109225.2891, Val Loss: 109440.5312\n",
      "Epoch [191/500], Train Loss: 109178.2422, Val Loss: 109423.1094\n",
      "Epoch [192/500], Train Loss: 109188.8203, Val Loss: 109406.1094\n",
      "Epoch [193/500], Train Loss: 109139.2969, Val Loss: 109389.4219\n",
      "Epoch [194/500], Train Loss: 109155.5078, Val Loss: 109373.0859\n",
      "Epoch [195/500], Train Loss: 109138.9922, Val Loss: 109357.0625\n",
      "Epoch [196/500], Train Loss: 109137.1406, Val Loss: 109341.3828\n",
      "Epoch [197/500], Train Loss: 109094.1094, Val Loss: 109326.0156\n",
      "Epoch [198/500], Train Loss: 109071.8359, Val Loss: 109310.9453\n",
      "Epoch [199/500], Train Loss: 109057.9766, Val Loss: 109296.1875\n",
      "Epoch [200/500], Train Loss: 109011.7812, Val Loss: 109281.6875\n",
      "Epoch [201/500], Train Loss: 109021.4141, Val Loss: 109267.4688\n",
      "Epoch [202/500], Train Loss: 109013.6562, Val Loss: 109253.4844\n",
      "Epoch [203/500], Train Loss: 108977.8203, Val Loss: 109239.7656\n",
      "Epoch [204/500], Train Loss: 108990.6875, Val Loss: 109226.3672\n",
      "Epoch [205/500], Train Loss: 109008.1953, Val Loss: 109213.1484\n",
      "Epoch [206/500], Train Loss: 108960.7891, Val Loss: 109200.2422\n",
      "Epoch [207/500], Train Loss: 108943.5000, Val Loss: 109187.5938\n",
      "Epoch [208/500], Train Loss: 108938.3828, Val Loss: 109175.1797\n",
      "Epoch [209/500], Train Loss: 108898.6719, Val Loss: 109162.9688\n",
      "Epoch [210/500], Train Loss: 108907.5938, Val Loss: 109151.0391\n",
      "Epoch [211/500], Train Loss: 108881.7266, Val Loss: 109139.2734\n",
      "Epoch [212/500], Train Loss: 108859.8203, Val Loss: 109127.6641\n",
      "Epoch [213/500], Train Loss: 108851.7812, Val Loss: 109116.3672\n",
      "Epoch [214/500], Train Loss: 108818.8438, Val Loss: 109105.2422\n",
      "Epoch [215/500], Train Loss: 108808.5078, Val Loss: 109094.2734\n",
      "Epoch [216/500], Train Loss: 108813.6562, Val Loss: 109083.6094\n",
      "Epoch [217/500], Train Loss: 108810.2891, Val Loss: 109073.1250\n",
      "Epoch [218/500], Train Loss: 108776.4531, Val Loss: 109062.8281\n",
      "Epoch [219/500], Train Loss: 108763.1328, Val Loss: 109052.8203\n",
      "Epoch [220/500], Train Loss: 108772.4453, Val Loss: 109042.9297\n",
      "Epoch [221/500], Train Loss: 108761.8828, Val Loss: 109033.2500\n",
      "Epoch [222/500], Train Loss: 108744.6250, Val Loss: 109023.8438\n",
      "Epoch [223/500], Train Loss: 108746.5391, Val Loss: 109014.5234\n",
      "Epoch [224/500], Train Loss: 108724.6250, Val Loss: 109005.4453\n",
      "Epoch [225/500], Train Loss: 108707.0234, Val Loss: 108996.6250\n",
      "Epoch [226/500], Train Loss: 108694.4375, Val Loss: 108987.9062\n",
      "Epoch [227/500], Train Loss: 108694.0156, Val Loss: 108979.4375\n",
      "Epoch [228/500], Train Loss: 108669.1484, Val Loss: 108971.1875\n",
      "Epoch [229/500], Train Loss: 108657.2891, Val Loss: 108963.0938\n",
      "Epoch [230/500], Train Loss: 108672.2109, Val Loss: 108955.2031\n",
      "Epoch [231/500], Train Loss: 108684.0234, Val Loss: 108947.5000\n",
      "Epoch [232/500], Train Loss: 108659.8984, Val Loss: 108939.9141\n",
      "Epoch [233/500], Train Loss: 108599.8984, Val Loss: 108932.5703\n",
      "Epoch [234/500], Train Loss: 108597.9922, Val Loss: 108925.3984\n",
      "Epoch [235/500], Train Loss: 108610.8438, Val Loss: 108918.2891\n",
      "Epoch [236/500], Train Loss: 108617.0391, Val Loss: 108911.4453\n",
      "Epoch [237/500], Train Loss: 108616.2656, Val Loss: 108904.7344\n",
      "Epoch [238/500], Train Loss: 108573.1094, Val Loss: 108898.1797\n",
      "Epoch [239/500], Train Loss: 108605.4453, Val Loss: 108891.8125\n",
      "Epoch [240/500], Train Loss: 108607.0469, Val Loss: 108885.5781\n",
      "Epoch [241/500], Train Loss: 108559.4922, Val Loss: 108879.4609\n",
      "Epoch [242/500], Train Loss: 108552.7109, Val Loss: 108873.5312\n",
      "Epoch [243/500], Train Loss: 108529.7578, Val Loss: 108867.6797\n",
      "Epoch [244/500], Train Loss: 108578.1406, Val Loss: 108861.9531\n",
      "Epoch [245/500], Train Loss: 108550.2656, Val Loss: 108856.4609\n",
      "Epoch [246/500], Train Loss: 108548.8516, Val Loss: 108851.0234\n",
      "Epoch [247/500], Train Loss: 108554.6562, Val Loss: 108845.7031\n",
      "Epoch [248/500], Train Loss: 108511.0078, Val Loss: 108840.4766\n",
      "Epoch [249/500], Train Loss: 108514.1016, Val Loss: 108835.3906\n",
      "Epoch [250/500], Train Loss: 108496.7891, Val Loss: 108830.5000\n",
      "Epoch [251/500], Train Loss: 108521.2656, Val Loss: 108825.6641\n",
      "Epoch [252/500], Train Loss: 108476.9375, Val Loss: 108820.9844\n",
      "Epoch [253/500], Train Loss: 108485.4609, Val Loss: 108816.5000\n",
      "Epoch [254/500], Train Loss: 108456.0859, Val Loss: 108812.0859\n",
      "Epoch [255/500], Train Loss: 108482.3984, Val Loss: 108807.7969\n",
      "Epoch [256/500], Train Loss: 108495.4375, Val Loss: 108803.6484\n",
      "Epoch [257/500], Train Loss: 108467.8828, Val Loss: 108799.5391\n",
      "Epoch [258/500], Train Loss: 108484.3594, Val Loss: 108795.5469\n",
      "Epoch [259/500], Train Loss: 108453.6562, Val Loss: 108791.6641\n",
      "Epoch [260/500], Train Loss: 108464.8594, Val Loss: 108787.8438\n",
      "Epoch [261/500], Train Loss: 108444.9297, Val Loss: 108784.1094\n",
      "Epoch [262/500], Train Loss: 108480.0391, Val Loss: 108780.4375\n",
      "Epoch [263/500], Train Loss: 108461.4688, Val Loss: 108776.9297\n",
      "Epoch [264/500], Train Loss: 108433.7734, Val Loss: 108773.5000\n",
      "Epoch [265/500], Train Loss: 108435.9297, Val Loss: 108770.2031\n",
      "Epoch [266/500], Train Loss: 108400.2891, Val Loss: 108766.9688\n",
      "Epoch [267/500], Train Loss: 108385.6016, Val Loss: 108763.7422\n",
      "Epoch [268/500], Train Loss: 108438.3438, Val Loss: 108760.6328\n",
      "Epoch [269/500], Train Loss: 108414.6094, Val Loss: 108757.5938\n",
      "Epoch [270/500], Train Loss: 108379.3906, Val Loss: 108754.6562\n",
      "Epoch [271/500], Train Loss: 108395.3438, Val Loss: 108751.8359\n",
      "Epoch [272/500], Train Loss: 108375.7031, Val Loss: 108749.0547\n",
      "Epoch [273/500], Train Loss: 108370.0469, Val Loss: 108746.3203\n",
      "Epoch [274/500], Train Loss: 108385.0469, Val Loss: 108743.6406\n",
      "Epoch [275/500], Train Loss: 108380.0469, Val Loss: 108741.0703\n",
      "Epoch [276/500], Train Loss: 108357.3516, Val Loss: 108738.5391\n",
      "Epoch [277/500], Train Loss: 108353.8594, Val Loss: 108736.0703\n",
      "Epoch [278/500], Train Loss: 108362.9922, Val Loss: 108733.7188\n",
      "Epoch [279/500], Train Loss: 108340.9141, Val Loss: 108731.3594\n",
      "Epoch [280/500], Train Loss: 108365.5000, Val Loss: 108729.1250\n",
      "Epoch [281/500], Train Loss: 108369.7578, Val Loss: 108726.9766\n",
      "Epoch [282/500], Train Loss: 108337.7109, Val Loss: 108724.7734\n",
      "Epoch [283/500], Train Loss: 108368.2812, Val Loss: 108722.6875\n",
      "Epoch [284/500], Train Loss: 108359.7969, Val Loss: 108720.6562\n",
      "Epoch [285/500], Train Loss: 108333.0078, Val Loss: 108718.6172\n",
      "Epoch [286/500], Train Loss: 108323.6406, Val Loss: 108716.7188\n",
      "Epoch [287/500], Train Loss: 108372.1953, Val Loss: 108714.9297\n",
      "Epoch [288/500], Train Loss: 108321.1641, Val Loss: 108713.1406\n",
      "Epoch [289/500], Train Loss: 108344.4531, Val Loss: 108711.3828\n",
      "Epoch [290/500], Train Loss: 108361.7344, Val Loss: 108709.7422\n",
      "Epoch [291/500], Train Loss: 108308.5859, Val Loss: 108708.0703\n",
      "Epoch [292/500], Train Loss: 108329.4062, Val Loss: 108706.4766\n",
      "Epoch [293/500], Train Loss: 108333.3047, Val Loss: 108705.0000\n",
      "Epoch [294/500], Train Loss: 108326.1953, Val Loss: 108703.5156\n",
      "Epoch [295/500], Train Loss: 108311.5000, Val Loss: 108702.0547\n",
      "Epoch [296/500], Train Loss: 108321.0000, Val Loss: 108700.6797\n",
      "Epoch [297/500], Train Loss: 108324.4531, Val Loss: 108699.3672\n",
      "Epoch [298/500], Train Loss: 108305.2500, Val Loss: 108698.0469\n",
      "Epoch [299/500], Train Loss: 108335.2109, Val Loss: 108696.7812\n",
      "Epoch [300/500], Train Loss: 108323.0391, Val Loss: 108695.5781\n",
      "Epoch [301/500], Train Loss: 108310.9375, Val Loss: 108694.3438\n",
      "Epoch [302/500], Train Loss: 108309.5234, Val Loss: 108693.1406\n",
      "Epoch [303/500], Train Loss: 108332.2344, Val Loss: 108691.9922\n",
      "Epoch [304/500], Train Loss: 108292.7188, Val Loss: 108690.8203\n",
      "Epoch [305/500], Train Loss: 108300.6797, Val Loss: 108689.7422\n",
      "Epoch [306/500], Train Loss: 108314.8125, Val Loss: 108688.6719\n",
      "Epoch [307/500], Train Loss: 108316.6719, Val Loss: 108687.6406\n",
      "Epoch [308/500], Train Loss: 108251.4688, Val Loss: 108686.6484\n",
      "Epoch [309/500], Train Loss: 108284.4766, Val Loss: 108685.7188\n",
      "Epoch [310/500], Train Loss: 108304.9922, Val Loss: 108684.8359\n",
      "Epoch [311/500], Train Loss: 108305.8594, Val Loss: 108683.9062\n",
      "Epoch [312/500], Train Loss: 108286.8750, Val Loss: 108683.0156\n",
      "Epoch [313/500], Train Loss: 108292.3438, Val Loss: 108682.2109\n",
      "Epoch [314/500], Train Loss: 108345.1562, Val Loss: 108681.3516\n",
      "Epoch [315/500], Train Loss: 108284.4219, Val Loss: 108680.5234\n",
      "Epoch [316/500], Train Loss: 108307.1562, Val Loss: 108679.7891\n",
      "Epoch [317/500], Train Loss: 108283.0781, Val Loss: 108679.0000\n",
      "Epoch [318/500], Train Loss: 108287.8828, Val Loss: 108678.2891\n",
      "Epoch [319/500], Train Loss: 108267.4922, Val Loss: 108677.6797\n",
      "Epoch [320/500], Train Loss: 108338.6953, Val Loss: 108677.0078\n",
      "Epoch [321/500], Train Loss: 108282.6719, Val Loss: 108676.4062\n",
      "Epoch [322/500], Train Loss: 108290.1016, Val Loss: 108675.8438\n",
      "Epoch [323/500], Train Loss: 108281.7266, Val Loss: 108675.2344\n",
      "Epoch [324/500], Train Loss: 108255.6250, Val Loss: 108674.6172\n",
      "Epoch [325/500], Train Loss: 108320.2109, Val Loss: 108674.0547\n",
      "Epoch [326/500], Train Loss: 108300.9141, Val Loss: 108673.5078\n",
      "Epoch [327/500], Train Loss: 108273.7266, Val Loss: 108673.0000\n",
      "Epoch [328/500], Train Loss: 108216.6328, Val Loss: 108672.5547\n",
      "Epoch [329/500], Train Loss: 108289.6875, Val Loss: 108672.0000\n",
      "Epoch [330/500], Train Loss: 108275.2188, Val Loss: 108671.4766\n",
      "Epoch [331/500], Train Loss: 108275.3047, Val Loss: 108670.9531\n",
      "Epoch [332/500], Train Loss: 108288.1406, Val Loss: 108670.4375\n",
      "Epoch [333/500], Train Loss: 108257.1562, Val Loss: 108670.0000\n",
      "Epoch [334/500], Train Loss: 108249.8281, Val Loss: 108669.4844\n",
      "Epoch [335/500], Train Loss: 108275.2266, Val Loss: 108669.0391\n",
      "Epoch [336/500], Train Loss: 108218.1953, Val Loss: 108668.6484\n",
      "Epoch [337/500], Train Loss: 108238.5859, Val Loss: 108668.2344\n",
      "Epoch [338/500], Train Loss: 108265.7734, Val Loss: 108667.8516\n",
      "Epoch [339/500], Train Loss: 108254.5391, Val Loss: 108667.5078\n",
      "Epoch [340/500], Train Loss: 108283.7422, Val Loss: 108667.1250\n",
      "Epoch [341/500], Train Loss: 108271.3047, Val Loss: 108666.8281\n",
      "Epoch [342/500], Train Loss: 108263.9531, Val Loss: 108666.5156\n",
      "Epoch [343/500], Train Loss: 108261.9219, Val Loss: 108666.2812\n",
      "Epoch [344/500], Train Loss: 108268.2031, Val Loss: 108665.9766\n",
      "Epoch [345/500], Train Loss: 108257.3047, Val Loss: 108665.7344\n",
      "Epoch [346/500], Train Loss: 108228.7422, Val Loss: 108665.4844\n",
      "Epoch [347/500], Train Loss: 108231.1797, Val Loss: 108665.2500\n",
      "Epoch [348/500], Train Loss: 108263.3281, Val Loss: 108665.0312\n",
      "Epoch [349/500], Train Loss: 108290.2188, Val Loss: 108664.8594\n",
      "Epoch [350/500], Train Loss: 108293.0078, Val Loss: 108664.6641\n",
      "Epoch [351/500], Train Loss: 108257.6172, Val Loss: 108664.4531\n",
      "Epoch [352/500], Train Loss: 108290.9531, Val Loss: 108664.2656\n",
      "Epoch [353/500], Train Loss: 108232.4297, Val Loss: 108664.0703\n",
      "Epoch [354/500], Train Loss: 108251.6172, Val Loss: 108663.8047\n",
      "Epoch [355/500], Train Loss: 108268.0547, Val Loss: 108663.5938\n",
      "Epoch [356/500], Train Loss: 108229.6328, Val Loss: 108663.3438\n",
      "Epoch [357/500], Train Loss: 108250.3984, Val Loss: 108663.0469\n",
      "Epoch [358/500], Train Loss: 108270.2734, Val Loss: 108662.8594\n",
      "Epoch [359/500], Train Loss: 108227.2891, Val Loss: 108662.6562\n",
      "Epoch [360/500], Train Loss: 108260.3125, Val Loss: 108662.4609\n",
      "Epoch [361/500], Train Loss: 108276.4609, Val Loss: 108662.2969\n",
      "Epoch [362/500], Train Loss: 108227.7500, Val Loss: 108662.1016\n",
      "Epoch [363/500], Train Loss: 108215.1484, Val Loss: 108661.9219\n",
      "Epoch [364/500], Train Loss: 108238.9219, Val Loss: 108661.8281\n",
      "Epoch [365/500], Train Loss: 108246.7812, Val Loss: 108661.6562\n",
      "Epoch [366/500], Train Loss: 108235.4844, Val Loss: 108661.5391\n",
      "Epoch [367/500], Train Loss: 108220.1953, Val Loss: 108661.4062\n",
      "Epoch [368/500], Train Loss: 108231.2656, Val Loss: 108661.2734\n",
      "Epoch [369/500], Train Loss: 108224.9297, Val Loss: 108661.1094\n",
      "Epoch [370/500], Train Loss: 108230.1953, Val Loss: 108660.9531\n",
      "Epoch [371/500], Train Loss: 108269.2422, Val Loss: 108660.8438\n",
      "Epoch [372/500], Train Loss: 108259.8984, Val Loss: 108660.7188\n",
      "Epoch [373/500], Train Loss: 108254.3516, Val Loss: 108660.6250\n",
      "Epoch [374/500], Train Loss: 108282.6641, Val Loss: 108660.5703\n",
      "Epoch [375/500], Train Loss: 108250.3828, Val Loss: 108660.4922\n",
      "Epoch [376/500], Train Loss: 108264.0547, Val Loss: 108660.4453\n",
      "Epoch [377/500], Train Loss: 108244.0391, Val Loss: 108660.3750\n",
      "Epoch [378/500], Train Loss: 108224.7422, Val Loss: 108660.2891\n",
      "Epoch [379/500], Train Loss: 108276.3906, Val Loss: 108660.1953\n",
      "Epoch [380/500], Train Loss: 108219.6875, Val Loss: 108660.1328\n",
      "Epoch [381/500], Train Loss: 108234.7969, Val Loss: 108660.0625\n",
      "Epoch [382/500], Train Loss: 108188.3438, Val Loss: 108659.9062\n",
      "Epoch [383/500], Train Loss: 108249.5078, Val Loss: 108659.7578\n",
      "Epoch [384/500], Train Loss: 108202.9844, Val Loss: 108659.6953\n",
      "Epoch [385/500], Train Loss: 108228.3203, Val Loss: 108659.5469\n",
      "Epoch [386/500], Train Loss: 108271.3281, Val Loss: 108659.4844\n",
      "Epoch [387/500], Train Loss: 108258.0078, Val Loss: 108659.4531\n",
      "Epoch [388/500], Train Loss: 108242.0156, Val Loss: 108659.3281\n",
      "Epoch [389/500], Train Loss: 108232.7109, Val Loss: 108659.2734\n",
      "Epoch [390/500], Train Loss: 108209.6562, Val Loss: 108659.1875\n",
      "Epoch [391/500], Train Loss: 108279.4688, Val Loss: 108659.0547\n",
      "Epoch [392/500], Train Loss: 108241.5781, Val Loss: 108659.0547\n",
      "Epoch [393/500], Train Loss: 108269.4844, Val Loss: 108659.0469\n",
      "Epoch [394/500], Train Loss: 108233.1953, Val Loss: 108659.0156\n",
      "Epoch [395/500], Train Loss: 108221.7969, Val Loss: 108659.0078\n",
      "Epoch [396/500], Train Loss: 108227.1641, Val Loss: 108658.9766\n",
      "Epoch [397/500], Train Loss: 108235.5391, Val Loss: 108658.8750\n",
      "Epoch [398/500], Train Loss: 108265.1250, Val Loss: 108658.8203\n",
      "Epoch [399/500], Train Loss: 108226.7422, Val Loss: 108658.7812\n",
      "Epoch [400/500], Train Loss: 108211.1719, Val Loss: 108658.7109\n",
      "Epoch [401/500], Train Loss: 108238.4453, Val Loss: 108658.6250\n",
      "Epoch [402/500], Train Loss: 108218.8828, Val Loss: 108658.5391\n",
      "Epoch [403/500], Train Loss: 108231.7812, Val Loss: 108658.4922\n",
      "Epoch [404/500], Train Loss: 108249.8359, Val Loss: 108658.3750\n",
      "Epoch [405/500], Train Loss: 108244.9375, Val Loss: 108658.3359\n",
      "Epoch [406/500], Train Loss: 108249.6641, Val Loss: 108658.2500\n",
      "Epoch [407/500], Train Loss: 108239.7812, Val Loss: 108658.1250\n",
      "Epoch [408/500], Train Loss: 108265.5547, Val Loss: 108658.0781\n",
      "Epoch [409/500], Train Loss: 108227.7344, Val Loss: 108658.0078\n",
      "Epoch [410/500], Train Loss: 108225.0703, Val Loss: 108657.9531\n",
      "Epoch [411/500], Train Loss: 108194.6797, Val Loss: 108657.8984\n",
      "Epoch [412/500], Train Loss: 108221.7656, Val Loss: 108657.8359\n",
      "Epoch [413/500], Train Loss: 108238.3516, Val Loss: 108657.7578\n",
      "Epoch [414/500], Train Loss: 108238.7734, Val Loss: 108657.7188\n",
      "Epoch [415/500], Train Loss: 108246.4453, Val Loss: 108657.6875\n",
      "Epoch [416/500], Train Loss: 108247.9141, Val Loss: 108657.6016\n",
      "Epoch [417/500], Train Loss: 108206.9062, Val Loss: 108657.5234\n",
      "Epoch [418/500], Train Loss: 108242.8516, Val Loss: 108657.5078\n",
      "Epoch [419/500], Train Loss: 108230.4531, Val Loss: 108657.5234\n",
      "Epoch [420/500], Train Loss: 108212.2969, Val Loss: 108657.4219\n",
      "Epoch [421/500], Train Loss: 108241.4766, Val Loss: 108657.3750\n",
      "Epoch [422/500], Train Loss: 108248.5234, Val Loss: 108657.3047\n",
      "Epoch [423/500], Train Loss: 108266.6484, Val Loss: 108657.2422\n",
      "Epoch [424/500], Train Loss: 108250.5312, Val Loss: 108657.2812\n",
      "Epoch [425/500], Train Loss: 108220.6016, Val Loss: 108657.1953\n",
      "Epoch [426/500], Train Loss: 108221.9688, Val Loss: 108657.0703\n",
      "Epoch [427/500], Train Loss: 108215.9766, Val Loss: 108657.0156\n",
      "Epoch [428/500], Train Loss: 108204.7109, Val Loss: 108656.8594\n",
      "Epoch [429/500], Train Loss: 108214.7734, Val Loss: 108656.7500\n",
      "Epoch [430/500], Train Loss: 108228.7578, Val Loss: 108656.6016\n",
      "Epoch [431/500], Train Loss: 108247.1016, Val Loss: 108656.4609\n",
      "Epoch [432/500], Train Loss: 108225.0391, Val Loss: 108656.2891\n",
      "Epoch [433/500], Train Loss: 108201.2812, Val Loss: 108656.2891\n",
      "Epoch [434/500], Train Loss: 108233.1953, Val Loss: 108656.3047\n",
      "Epoch [435/500], Train Loss: 108208.0234, Val Loss: 108656.2344\n",
      "Epoch [436/500], Train Loss: 108227.9922, Val Loss: 108656.2500\n",
      "Epoch [437/500], Train Loss: 108235.1406, Val Loss: 108656.2344\n",
      "Epoch [438/500], Train Loss: 108223.6016, Val Loss: 108656.2266\n",
      "Epoch [439/500], Train Loss: 108236.4375, Val Loss: 108656.2812\n",
      "Epoch [440/500], Train Loss: 108215.2188, Val Loss: 108656.3359\n",
      "Epoch [441/500], Train Loss: 108232.2422, Val Loss: 108656.3125\n",
      "Epoch [442/500], Train Loss: 108236.8672, Val Loss: 108656.3672\n",
      "Epoch [443/500], Train Loss: 108237.7188, Val Loss: 108656.4297\n",
      "Early stopping...\n",
      "Test Loss: 107604.4297\n",
      "Epoch [1/500], Train Loss: 135366.4531, Val Loss: 135961.1094\n",
      "Epoch [2/500], Train Loss: 135366.1094, Val Loss: 135960.7500\n",
      "Epoch [3/500], Train Loss: 135365.7188, Val Loss: 135960.3438\n",
      "Epoch [4/500], Train Loss: 135365.3594, Val Loss: 135959.9531\n",
      "Epoch [5/500], Train Loss: 135364.9688, Val Loss: 135959.5625\n",
      "Epoch [6/500], Train Loss: 135364.5625, Val Loss: 135959.1250\n",
      "Epoch [7/500], Train Loss: 135364.1406, Val Loss: 135958.7031\n",
      "Epoch [8/500], Train Loss: 135363.7188, Val Loss: 135958.2344\n",
      "Epoch [9/500], Train Loss: 135363.2812, Val Loss: 135957.7656\n",
      "Epoch [10/500], Train Loss: 135362.7969, Val Loss: 135957.2656\n",
      "Epoch [11/500], Train Loss: 135362.3125, Val Loss: 135956.7500\n",
      "Epoch [12/500], Train Loss: 135361.8125, Val Loss: 135956.1875\n",
      "Epoch [13/500], Train Loss: 135361.2500, Val Loss: 135955.6250\n",
      "Epoch [14/500], Train Loss: 135360.6719, Val Loss: 135954.9844\n",
      "Epoch [15/500], Train Loss: 135360.0625, Val Loss: 135954.2969\n",
      "Epoch [16/500], Train Loss: 135359.3594, Val Loss: 135953.5781\n",
      "Epoch [17/500], Train Loss: 135358.6250, Val Loss: 135952.7344\n",
      "Epoch [18/500], Train Loss: 135357.8281, Val Loss: 135951.8438\n",
      "Epoch [19/500], Train Loss: 135356.9062, Val Loss: 135950.8438\n",
      "Epoch [20/500], Train Loss: 135355.9062, Val Loss: 135949.7188\n",
      "Epoch [21/500], Train Loss: 135354.7812, Val Loss: 135948.4688\n",
      "Epoch [22/500], Train Loss: 135353.5469, Val Loss: 135947.0312\n",
      "Epoch [23/500], Train Loss: 135352.1250, Val Loss: 135945.4688\n",
      "Epoch [24/500], Train Loss: 135350.5312, Val Loss: 135943.6719\n",
      "Epoch [25/500], Train Loss: 135348.7500, Val Loss: 135941.6250\n",
      "Epoch [26/500], Train Loss: 135346.7188, Val Loss: 135939.3281\n",
      "Epoch [27/500], Train Loss: 135344.4688, Val Loss: 135936.7344\n",
      "Epoch [28/500], Train Loss: 135341.9062, Val Loss: 135933.8125\n",
      "Epoch [29/500], Train Loss: 135339.0000, Val Loss: 135930.5156\n",
      "Epoch [30/500], Train Loss: 135335.7656, Val Loss: 135926.8125\n",
      "Epoch [31/500], Train Loss: 135332.1875, Val Loss: 135922.6875\n",
      "Epoch [32/500], Train Loss: 135328.1094, Val Loss: 135918.0781\n",
      "Epoch [33/500], Train Loss: 135323.5938, Val Loss: 135912.9375\n",
      "Epoch [34/500], Train Loss: 135318.5000, Val Loss: 135907.2188\n",
      "Epoch [35/500], Train Loss: 135312.8594, Val Loss: 135900.8281\n",
      "Epoch [36/500], Train Loss: 135306.4531, Val Loss: 135893.7500\n",
      "Epoch [37/500], Train Loss: 135299.5000, Val Loss: 135885.8594\n",
      "Epoch [38/500], Train Loss: 135291.7344, Val Loss: 135877.1094\n",
      "Epoch [39/500], Train Loss: 135283.1250, Val Loss: 135867.3906\n",
      "Epoch [40/500], Train Loss: 135273.5000, Val Loss: 135856.6562\n",
      "Epoch [41/500], Train Loss: 135263.0000, Val Loss: 135844.7969\n",
      "Epoch [42/500], Train Loss: 135251.2500, Val Loss: 135831.7188\n",
      "Epoch [43/500], Train Loss: 135238.3125, Val Loss: 135817.3438\n",
      "Epoch [44/500], Train Loss: 135224.2656, Val Loss: 135801.5625\n",
      "Epoch [45/500], Train Loss: 135208.7188, Val Loss: 135784.2344\n",
      "Epoch [46/500], Train Loss: 135191.4688, Val Loss: 135765.2500\n",
      "Epoch [47/500], Train Loss: 135172.8594, Val Loss: 135744.4375\n",
      "Epoch [48/500], Train Loss: 135152.2344, Val Loss: 135721.6562\n",
      "Epoch [49/500], Train Loss: 135129.7031, Val Loss: 135696.7812\n",
      "Epoch [50/500], Train Loss: 135105.0000, Val Loss: 135669.6875\n",
      "Epoch [51/500], Train Loss: 135078.3125, Val Loss: 135640.1250\n",
      "Epoch [52/500], Train Loss: 135049.3281, Val Loss: 135608.0312\n",
      "Epoch [53/500], Train Loss: 135017.7344, Val Loss: 135573.1875\n",
      "Epoch [54/500], Train Loss: 134983.0625, Val Loss: 135535.4375\n",
      "Epoch [55/500], Train Loss: 134945.6719, Val Loss: 135494.5781\n",
      "Epoch [56/500], Train Loss: 134905.7812, Val Loss: 135450.3281\n",
      "Epoch [57/500], Train Loss: 134862.3906, Val Loss: 135402.5938\n",
      "Epoch [58/500], Train Loss: 134814.5312, Val Loss: 135351.0938\n",
      "Epoch [59/500], Train Loss: 134763.9062, Val Loss: 135295.6406\n",
      "Epoch [60/500], Train Loss: 134709.4375, Val Loss: 135236.0156\n",
      "Epoch [61/500], Train Loss: 134650.2969, Val Loss: 135171.9844\n",
      "Epoch [62/500], Train Loss: 134587.6094, Val Loss: 135103.3125\n",
      "Epoch [63/500], Train Loss: 134519.4219, Val Loss: 135029.7656\n",
      "Epoch [64/500], Train Loss: 134446.0156, Val Loss: 134951.0938\n",
      "Epoch [65/500], Train Loss: 134369.4219, Val Loss: 134867.0156\n",
      "Epoch [66/500], Train Loss: 134286.5781, Val Loss: 134777.2812\n",
      "Epoch [67/500], Train Loss: 134197.5781, Val Loss: 134681.6719\n",
      "Epoch [68/500], Train Loss: 134104.2812, Val Loss: 134579.8750\n",
      "Epoch [69/500], Train Loss: 134003.2656, Val Loss: 134471.6406\n",
      "Epoch [70/500], Train Loss: 133896.7188, Val Loss: 134356.7344\n",
      "Epoch [71/500], Train Loss: 133782.1406, Val Loss: 134234.9062\n",
      "Epoch [72/500], Train Loss: 133660.6719, Val Loss: 134105.8750\n",
      "Epoch [73/500], Train Loss: 133534.0312, Val Loss: 133969.4062\n",
      "Epoch [74/500], Train Loss: 133398.9062, Val Loss: 133825.2344\n",
      "Epoch [75/500], Train Loss: 133258.0156, Val Loss: 133673.1719\n",
      "Epoch [76/500], Train Loss: 133105.0625, Val Loss: 133512.9375\n",
      "Epoch [77/500], Train Loss: 132949.3906, Val Loss: 133344.3594\n",
      "Epoch [78/500], Train Loss: 132781.2188, Val Loss: 133167.2031\n",
      "Epoch [79/500], Train Loss: 132609.1719, Val Loss: 132981.2812\n",
      "Epoch [80/500], Train Loss: 132423.7344, Val Loss: 132786.4062\n",
      "Epoch [81/500], Train Loss: 132229.3125, Val Loss: 132582.4219\n",
      "Epoch [82/500], Train Loss: 132028.6719, Val Loss: 132369.1562\n",
      "Epoch [83/500], Train Loss: 131816.5469, Val Loss: 132146.5156\n",
      "Epoch [84/500], Train Loss: 131597.4062, Val Loss: 131914.4375\n",
      "Epoch [85/500], Train Loss: 131363.1562, Val Loss: 131672.8281\n",
      "Epoch [86/500], Train Loss: 131121.7656, Val Loss: 131421.6406\n",
      "Epoch [87/500], Train Loss: 130876.4297, Val Loss: 131160.8906\n",
      "Epoch [88/500], Train Loss: 130619.3203, Val Loss: 130890.6406\n",
      "Epoch [89/500], Train Loss: 130351.9375, Val Loss: 130610.9688\n",
      "Epoch [90/500], Train Loss: 130078.8281, Val Loss: 130322.0000\n",
      "Epoch [91/500], Train Loss: 129786.4375, Val Loss: 130023.9062\n",
      "Epoch [92/500], Train Loss: 129491.6016, Val Loss: 129716.8438\n",
      "Epoch [93/500], Train Loss: 129181.2969, Val Loss: 129401.1016\n",
      "Epoch [94/500], Train Loss: 128871.2500, Val Loss: 129077.0156\n",
      "Epoch [95/500], Train Loss: 128546.4219, Val Loss: 128744.9297\n",
      "Epoch [96/500], Train Loss: 128214.5781, Val Loss: 128405.3203\n",
      "Epoch [97/500], Train Loss: 127867.5000, Val Loss: 128058.6875\n",
      "Epoch [98/500], Train Loss: 127540.5234, Val Loss: 127705.6250\n",
      "Epoch [99/500], Train Loss: 127182.9531, Val Loss: 127346.6641\n",
      "Epoch [100/500], Train Loss: 126822.2031, Val Loss: 126982.3672\n",
      "Epoch [101/500], Train Loss: 126463.9062, Val Loss: 126613.4688\n",
      "Epoch [102/500], Train Loss: 126093.5078, Val Loss: 126240.6562\n",
      "Epoch [103/500], Train Loss: 125711.1641, Val Loss: 125864.6562\n",
      "Epoch [104/500], Train Loss: 125361.1641, Val Loss: 125486.2109\n",
      "Epoch [105/500], Train Loss: 124971.9688, Val Loss: 125106.1016\n",
      "Epoch [106/500], Train Loss: 124591.7188, Val Loss: 124725.0234\n",
      "Epoch [107/500], Train Loss: 124218.5781, Val Loss: 124343.8359\n",
      "Epoch [108/500], Train Loss: 123834.5000, Val Loss: 123963.3203\n",
      "Epoch [109/500], Train Loss: 123449.8281, Val Loss: 123584.3438\n",
      "Epoch [110/500], Train Loss: 123086.2422, Val Loss: 123207.8047\n",
      "Epoch [111/500], Train Loss: 122694.2266, Val Loss: 122834.6484\n",
      "Epoch [112/500], Train Loss: 122326.8828, Val Loss: 122465.7578\n",
      "Epoch [113/500], Train Loss: 121962.2188, Val Loss: 122102.0781\n",
      "Epoch [114/500], Train Loss: 121595.9219, Val Loss: 121744.4922\n",
      "Epoch [115/500], Train Loss: 121229.3594, Val Loss: 121393.7656\n",
      "Epoch [116/500], Train Loss: 120883.9922, Val Loss: 121050.7109\n",
      "Epoch [117/500], Train Loss: 120520.0000, Val Loss: 120715.9531\n",
      "Epoch [118/500], Train Loss: 120209.0547, Val Loss: 120390.1094\n",
      "Epoch [119/500], Train Loss: 119883.9688, Val Loss: 120073.6719\n",
      "Epoch [120/500], Train Loss: 119563.3672, Val Loss: 119767.0078\n",
      "Epoch [121/500], Train Loss: 119262.9531, Val Loss: 119470.4766\n",
      "Epoch [122/500], Train Loss: 118967.6719, Val Loss: 119184.1094\n",
      "Epoch [123/500], Train Loss: 118689.9844, Val Loss: 118907.9453\n",
      "Epoch [124/500], Train Loss: 118402.7578, Val Loss: 118642.0156\n",
      "Epoch [125/500], Train Loss: 118116.6250, Val Loss: 118385.9141\n",
      "Epoch [126/500], Train Loss: 117880.5312, Val Loss: 118139.3438\n",
      "Epoch [127/500], Train Loss: 117632.3359, Val Loss: 117902.0703\n",
      "Epoch [128/500], Train Loss: 117414.3906, Val Loss: 117673.4922\n",
      "Epoch [129/500], Train Loss: 117173.2734, Val Loss: 117453.0781\n",
      "Epoch [130/500], Train Loss: 116958.6094, Val Loss: 117240.3828\n",
      "Epoch [131/500], Train Loss: 116747.3750, Val Loss: 117034.7109\n",
      "Epoch [132/500], Train Loss: 116546.1406, Val Loss: 116835.4609\n",
      "Epoch [133/500], Train Loss: 116368.8672, Val Loss: 116642.0469\n",
      "Epoch [134/500], Train Loss: 116178.9844, Val Loss: 116453.8047\n",
      "Epoch [135/500], Train Loss: 115996.3828, Val Loss: 116270.3828\n",
      "Epoch [136/500], Train Loss: 115807.3594, Val Loss: 116091.2422\n",
      "Epoch [137/500], Train Loss: 115650.5391, Val Loss: 115915.9844\n",
      "Epoch [138/500], Train Loss: 115477.2812, Val Loss: 115744.2891\n",
      "Epoch [139/500], Train Loss: 115315.8359, Val Loss: 115575.8828\n",
      "Epoch [140/500], Train Loss: 115123.5859, Val Loss: 115410.6562\n",
      "Epoch [141/500], Train Loss: 114984.3203, Val Loss: 115248.4922\n",
      "Epoch [142/500], Train Loss: 114833.5938, Val Loss: 115089.3750\n",
      "Epoch [143/500], Train Loss: 114651.4531, Val Loss: 114933.4219\n",
      "Epoch [144/500], Train Loss: 114513.9062, Val Loss: 114780.5938\n",
      "Epoch [145/500], Train Loss: 114386.1094, Val Loss: 114631.0234\n",
      "Epoch [146/500], Train Loss: 114246.3906, Val Loss: 114484.8906\n",
      "Epoch [147/500], Train Loss: 114104.9453, Val Loss: 114342.1250\n",
      "Epoch [148/500], Train Loss: 113987.4531, Val Loss: 114202.8750\n",
      "Epoch [149/500], Train Loss: 113845.7344, Val Loss: 114067.0859\n",
      "Epoch [150/500], Train Loss: 113721.1094, Val Loss: 113934.8125\n",
      "Epoch [151/500], Train Loss: 113582.6094, Val Loss: 113806.1641\n",
      "Epoch [152/500], Train Loss: 113473.7734, Val Loss: 113680.9531\n",
      "Epoch [153/500], Train Loss: 113354.2891, Val Loss: 113559.2734\n",
      "Epoch [154/500], Train Loss: 113230.1719, Val Loss: 113441.1484\n",
      "Epoch [155/500], Train Loss: 113138.0781, Val Loss: 113326.4375\n",
      "Epoch [156/500], Train Loss: 113027.5234, Val Loss: 113215.1016\n",
      "Epoch [157/500], Train Loss: 112896.3359, Val Loss: 113107.1406\n",
      "Epoch [158/500], Train Loss: 112798.8594, Val Loss: 113002.4453\n",
      "Epoch [159/500], Train Loss: 112703.9453, Val Loss: 112901.0000\n",
      "Epoch [160/500], Train Loss: 112597.3906, Val Loss: 112802.6719\n",
      "Epoch [161/500], Train Loss: 112505.7500, Val Loss: 112707.4297\n",
      "Epoch [162/500], Train Loss: 112421.7344, Val Loss: 112615.1328\n",
      "Epoch [163/500], Train Loss: 112345.7422, Val Loss: 112525.7578\n",
      "Epoch [164/500], Train Loss: 112229.8359, Val Loss: 112439.1250\n",
      "Epoch [165/500], Train Loss: 112136.3906, Val Loss: 112355.1953\n",
      "Epoch [166/500], Train Loss: 112095.3438, Val Loss: 112273.9219\n",
      "Epoch [167/500], Train Loss: 112005.3672, Val Loss: 112195.1562\n",
      "Epoch [168/500], Train Loss: 111954.0391, Val Loss: 112118.8750\n",
      "Epoch [169/500], Train Loss: 111865.9688, Val Loss: 112045.0234\n",
      "Epoch [170/500], Train Loss: 111788.8672, Val Loss: 111973.3984\n",
      "Epoch [171/500], Train Loss: 111716.5156, Val Loss: 111904.0391\n",
      "Epoch [172/500], Train Loss: 111691.2344, Val Loss: 111836.7969\n",
      "Epoch [173/500], Train Loss: 111607.4375, Val Loss: 111771.5625\n",
      "Epoch [174/500], Train Loss: 111546.6328, Val Loss: 111708.3594\n",
      "Epoch [175/500], Train Loss: 111456.7500, Val Loss: 111647.0234\n",
      "Epoch [176/500], Train Loss: 111438.0703, Val Loss: 111587.4688\n",
      "Epoch [177/500], Train Loss: 111365.9297, Val Loss: 111529.7500\n",
      "Epoch [178/500], Train Loss: 111299.6172, Val Loss: 111473.6953\n",
      "Epoch [179/500], Train Loss: 111243.6172, Val Loss: 111419.2422\n",
      "Epoch [180/500], Train Loss: 111196.6641, Val Loss: 111366.4297\n",
      "Epoch [181/500], Train Loss: 111172.9062, Val Loss: 111315.0938\n",
      "Epoch [182/500], Train Loss: 111099.2188, Val Loss: 111265.2031\n",
      "Epoch [183/500], Train Loss: 111078.5391, Val Loss: 111216.7344\n",
      "Epoch [184/500], Train Loss: 111024.3516, Val Loss: 111169.5469\n",
      "Epoch [185/500], Train Loss: 110970.1016, Val Loss: 111123.6797\n",
      "Epoch [186/500], Train Loss: 110923.3281, Val Loss: 111079.0625\n",
      "Epoch [187/500], Train Loss: 110896.5703, Val Loss: 111035.5547\n",
      "Epoch [188/500], Train Loss: 110835.0000, Val Loss: 110993.2344\n",
      "Epoch [189/500], Train Loss: 110819.4688, Val Loss: 110951.9844\n",
      "Epoch [190/500], Train Loss: 110766.5078, Val Loss: 110911.8047\n",
      "Epoch [191/500], Train Loss: 110713.3438, Val Loss: 110872.6016\n",
      "Epoch [192/500], Train Loss: 110696.3281, Val Loss: 110834.3750\n",
      "Epoch [193/500], Train Loss: 110643.0391, Val Loss: 110797.0469\n",
      "Epoch [194/500], Train Loss: 110596.7422, Val Loss: 110760.6094\n",
      "Epoch [195/500], Train Loss: 110573.9375, Val Loss: 110725.0312\n",
      "Epoch [196/500], Train Loss: 110519.9375, Val Loss: 110690.1875\n",
      "Epoch [197/500], Train Loss: 110491.2344, Val Loss: 110656.1719\n",
      "Epoch [198/500], Train Loss: 110475.4219, Val Loss: 110622.9453\n",
      "Epoch [199/500], Train Loss: 110434.0156, Val Loss: 110590.3828\n",
      "Epoch [200/500], Train Loss: 110397.2969, Val Loss: 110558.5312\n",
      "Epoch [201/500], Train Loss: 110374.4766, Val Loss: 110527.3828\n",
      "Epoch [202/500], Train Loss: 110367.1250, Val Loss: 110496.8984\n",
      "Epoch [203/500], Train Loss: 110329.0391, Val Loss: 110467.0781\n",
      "Epoch [204/500], Train Loss: 110268.0000, Val Loss: 110437.8359\n",
      "Epoch [205/500], Train Loss: 110216.5625, Val Loss: 110409.2266\n",
      "Epoch [206/500], Train Loss: 110240.2500, Val Loss: 110381.2109\n",
      "Epoch [207/500], Train Loss: 110200.3203, Val Loss: 110353.7031\n",
      "Epoch [208/500], Train Loss: 110161.1719, Val Loss: 110326.7734\n",
      "Epoch [209/500], Train Loss: 110158.2812, Val Loss: 110300.3516\n",
      "Epoch [210/500], Train Loss: 110105.4609, Val Loss: 110274.4688\n",
      "Epoch [211/500], Train Loss: 110093.7578, Val Loss: 110249.0391\n",
      "Epoch [212/500], Train Loss: 110057.8125, Val Loss: 110224.1719\n",
      "Epoch [213/500], Train Loss: 110042.1797, Val Loss: 110199.7031\n",
      "Epoch [214/500], Train Loss: 109990.5469, Val Loss: 110175.6328\n",
      "Epoch [215/500], Train Loss: 109984.9453, Val Loss: 110152.0312\n",
      "Epoch [216/500], Train Loss: 109962.4219, Val Loss: 110128.8047\n",
      "Epoch [217/500], Train Loss: 109967.3828, Val Loss: 110106.0469\n",
      "Epoch [218/500], Train Loss: 109901.3125, Val Loss: 110083.6953\n",
      "Epoch [219/500], Train Loss: 109902.6562, Val Loss: 110061.7031\n",
      "Epoch [220/500], Train Loss: 109840.2109, Val Loss: 110040.0547\n",
      "Epoch [221/500], Train Loss: 109816.0859, Val Loss: 110018.7734\n",
      "Epoch [222/500], Train Loss: 109828.4766, Val Loss: 109997.8281\n",
      "Epoch [223/500], Train Loss: 109817.4453, Val Loss: 109977.2266\n",
      "Epoch [224/500], Train Loss: 109770.8047, Val Loss: 109957.0312\n",
      "Epoch [225/500], Train Loss: 109771.2500, Val Loss: 109937.1094\n",
      "Epoch [226/500], Train Loss: 109765.8672, Val Loss: 109917.4844\n",
      "Epoch [227/500], Train Loss: 109717.8594, Val Loss: 109898.2344\n",
      "Epoch [228/500], Train Loss: 109717.3281, Val Loss: 109879.3047\n",
      "Epoch [229/500], Train Loss: 109681.5938, Val Loss: 109860.6562\n",
      "Epoch [230/500], Train Loss: 109630.1719, Val Loss: 109842.2891\n",
      "Epoch [231/500], Train Loss: 109646.0156, Val Loss: 109824.1797\n",
      "Epoch [232/500], Train Loss: 109628.4844, Val Loss: 109806.3438\n",
      "Epoch [233/500], Train Loss: 109576.8594, Val Loss: 109788.7734\n",
      "Epoch [234/500], Train Loss: 109570.2188, Val Loss: 109771.4453\n",
      "Epoch [235/500], Train Loss: 109557.0547, Val Loss: 109754.3984\n",
      "Epoch [236/500], Train Loss: 109546.0781, Val Loss: 109737.5312\n",
      "Epoch [237/500], Train Loss: 109499.6484, Val Loss: 109721.0078\n",
      "Epoch [238/500], Train Loss: 109506.1016, Val Loss: 109704.6797\n",
      "Epoch [239/500], Train Loss: 109499.4297, Val Loss: 109688.5781\n",
      "Epoch [240/500], Train Loss: 109447.2656, Val Loss: 109672.7656\n",
      "Epoch [241/500], Train Loss: 109459.6016, Val Loss: 109657.1562\n",
      "Epoch [242/500], Train Loss: 109437.1016, Val Loss: 109641.7812\n",
      "Epoch [243/500], Train Loss: 109448.3359, Val Loss: 109626.6484\n",
      "Epoch [244/500], Train Loss: 109387.9531, Val Loss: 109611.7031\n",
      "Epoch [245/500], Train Loss: 109384.7188, Val Loss: 109597.0000\n",
      "Epoch [246/500], Train Loss: 109378.9609, Val Loss: 109582.5156\n",
      "Epoch [247/500], Train Loss: 109361.7578, Val Loss: 109568.2031\n",
      "Epoch [248/500], Train Loss: 109331.9453, Val Loss: 109554.1016\n",
      "Epoch [249/500], Train Loss: 109284.5781, Val Loss: 109540.1641\n",
      "Epoch [250/500], Train Loss: 109294.4141, Val Loss: 109526.3828\n",
      "Epoch [251/500], Train Loss: 109303.5469, Val Loss: 109512.8359\n",
      "Epoch [252/500], Train Loss: 109272.1484, Val Loss: 109499.4766\n",
      "Epoch [253/500], Train Loss: 109291.3281, Val Loss: 109486.2891\n",
      "Epoch [254/500], Train Loss: 109262.7969, Val Loss: 109473.3203\n",
      "Epoch [255/500], Train Loss: 109252.8203, Val Loss: 109460.5078\n",
      "Epoch [256/500], Train Loss: 109238.5547, Val Loss: 109447.8672\n",
      "Epoch [257/500], Train Loss: 109205.4141, Val Loss: 109435.3906\n",
      "Epoch [258/500], Train Loss: 109190.5781, Val Loss: 109423.0938\n",
      "Epoch [259/500], Train Loss: 109148.8516, Val Loss: 109410.9688\n",
      "Epoch [260/500], Train Loss: 109143.1953, Val Loss: 109398.9766\n",
      "Epoch [261/500], Train Loss: 109157.7734, Val Loss: 109387.1641\n",
      "Epoch [262/500], Train Loss: 109133.1484, Val Loss: 109375.4844\n",
      "Epoch [263/500], Train Loss: 109124.6328, Val Loss: 109363.9531\n",
      "Epoch [264/500], Train Loss: 109109.6094, Val Loss: 109352.6250\n",
      "Epoch [265/500], Train Loss: 109100.3984, Val Loss: 109341.4688\n",
      "Epoch [266/500], Train Loss: 109116.1094, Val Loss: 109330.4219\n",
      "Epoch [267/500], Train Loss: 109098.5234, Val Loss: 109319.5391\n",
      "Epoch [268/500], Train Loss: 109060.3203, Val Loss: 109308.8281\n",
      "Epoch [269/500], Train Loss: 109048.0469, Val Loss: 109298.2188\n",
      "Epoch [270/500], Train Loss: 109044.2891, Val Loss: 109287.7266\n",
      "Epoch [271/500], Train Loss: 109029.6406, Val Loss: 109277.4688\n",
      "Epoch [272/500], Train Loss: 109017.8750, Val Loss: 109267.2969\n",
      "Epoch [273/500], Train Loss: 109023.4375, Val Loss: 109257.2734\n",
      "Epoch [274/500], Train Loss: 108957.9844, Val Loss: 109247.4219\n",
      "Epoch [275/500], Train Loss: 108969.8281, Val Loss: 109237.7031\n",
      "Epoch [276/500], Train Loss: 108978.7031, Val Loss: 109228.0781\n",
      "Epoch [277/500], Train Loss: 108939.8359, Val Loss: 109218.6406\n",
      "Epoch [278/500], Train Loss: 108962.4219, Val Loss: 109209.2969\n",
      "Epoch [279/500], Train Loss: 108898.7422, Val Loss: 109200.0938\n",
      "Epoch [280/500], Train Loss: 108934.7031, Val Loss: 109191.0078\n",
      "Epoch [281/500], Train Loss: 108903.2031, Val Loss: 109182.1016\n",
      "Epoch [282/500], Train Loss: 108909.7031, Val Loss: 109173.2422\n",
      "Epoch [283/500], Train Loss: 108877.1875, Val Loss: 109164.5312\n",
      "Epoch [284/500], Train Loss: 108919.4688, Val Loss: 109155.9844\n",
      "Epoch [285/500], Train Loss: 108861.3438, Val Loss: 109147.5156\n",
      "Epoch [286/500], Train Loss: 108848.7656, Val Loss: 109139.1562\n",
      "Epoch [287/500], Train Loss: 108844.9297, Val Loss: 109130.9844\n",
      "Epoch [288/500], Train Loss: 108846.6406, Val Loss: 109122.8984\n",
      "Epoch [289/500], Train Loss: 108844.2969, Val Loss: 109114.9297\n",
      "Epoch [290/500], Train Loss: 108832.1797, Val Loss: 109107.1484\n",
      "Epoch [291/500], Train Loss: 108841.4688, Val Loss: 109099.3828\n",
      "Epoch [292/500], Train Loss: 108776.9766, Val Loss: 109091.7578\n",
      "Epoch [293/500], Train Loss: 108787.1094, Val Loss: 109084.2734\n",
      "Epoch [294/500], Train Loss: 108788.5156, Val Loss: 109076.7969\n",
      "Epoch [295/500], Train Loss: 108794.8438, Val Loss: 109069.4688\n",
      "Epoch [296/500], Train Loss: 108774.6875, Val Loss: 109062.3047\n",
      "Epoch [297/500], Train Loss: 108742.1328, Val Loss: 109055.2266\n",
      "Epoch [298/500], Train Loss: 108743.6953, Val Loss: 109048.2188\n",
      "Epoch [299/500], Train Loss: 108758.6172, Val Loss: 109041.4297\n",
      "Epoch [300/500], Train Loss: 108762.2812, Val Loss: 109034.6797\n",
      "Epoch [301/500], Train Loss: 108759.7031, Val Loss: 109028.0469\n",
      "Epoch [302/500], Train Loss: 108749.9844, Val Loss: 109021.5234\n",
      "Epoch [303/500], Train Loss: 108702.0156, Val Loss: 109015.0781\n",
      "Epoch [304/500], Train Loss: 108700.6641, Val Loss: 109008.7109\n",
      "Epoch [305/500], Train Loss: 108727.7031, Val Loss: 109002.4844\n",
      "Epoch [306/500], Train Loss: 108717.8594, Val Loss: 108996.3672\n",
      "Epoch [307/500], Train Loss: 108645.8594, Val Loss: 108990.2500\n",
      "Epoch [308/500], Train Loss: 108685.8906, Val Loss: 108984.2500\n",
      "Epoch [309/500], Train Loss: 108644.3594, Val Loss: 108978.3281\n",
      "Epoch [310/500], Train Loss: 108653.3125, Val Loss: 108972.4766\n",
      "Epoch [311/500], Train Loss: 108658.7109, Val Loss: 108966.7422\n",
      "Epoch [312/500], Train Loss: 108637.6250, Val Loss: 108961.0938\n",
      "Epoch [313/500], Train Loss: 108649.1562, Val Loss: 108955.5547\n",
      "Epoch [314/500], Train Loss: 108647.0000, Val Loss: 108950.1094\n",
      "Epoch [315/500], Train Loss: 108631.2734, Val Loss: 108944.7578\n",
      "Epoch [316/500], Train Loss: 108633.7109, Val Loss: 108939.4922\n",
      "Epoch [317/500], Train Loss: 108610.1797, Val Loss: 108934.3203\n",
      "Epoch [318/500], Train Loss: 108608.3594, Val Loss: 108929.1953\n",
      "Epoch [319/500], Train Loss: 108610.0312, Val Loss: 108924.1953\n",
      "Epoch [320/500], Train Loss: 108591.6484, Val Loss: 108919.2578\n",
      "Epoch [321/500], Train Loss: 108590.5469, Val Loss: 108914.3750\n",
      "Epoch [322/500], Train Loss: 108603.2031, Val Loss: 108909.6484\n",
      "Epoch [323/500], Train Loss: 108569.9219, Val Loss: 108904.9922\n",
      "Epoch [324/500], Train Loss: 108549.5625, Val Loss: 108900.3516\n",
      "Epoch [325/500], Train Loss: 108607.8672, Val Loss: 108895.8281\n",
      "Epoch [326/500], Train Loss: 108587.9609, Val Loss: 108891.3359\n",
      "Epoch [327/500], Train Loss: 108578.7891, Val Loss: 108886.9062\n",
      "Epoch [328/500], Train Loss: 108563.4844, Val Loss: 108882.5859\n",
      "Epoch [329/500], Train Loss: 108544.1406, Val Loss: 108878.3203\n",
      "Epoch [330/500], Train Loss: 108534.4297, Val Loss: 108874.1094\n",
      "Epoch [331/500], Train Loss: 108553.8672, Val Loss: 108869.9531\n",
      "Epoch [332/500], Train Loss: 108525.5156, Val Loss: 108865.8984\n",
      "Epoch [333/500], Train Loss: 108504.7031, Val Loss: 108861.8672\n",
      "Epoch [334/500], Train Loss: 108534.2656, Val Loss: 108857.9297\n",
      "Epoch [335/500], Train Loss: 108513.1953, Val Loss: 108854.0703\n",
      "Epoch [336/500], Train Loss: 108477.8203, Val Loss: 108850.2578\n",
      "Epoch [337/500], Train Loss: 108536.3594, Val Loss: 108846.5156\n",
      "Epoch [338/500], Train Loss: 108484.7188, Val Loss: 108842.8125\n",
      "Epoch [339/500], Train Loss: 108512.1953, Val Loss: 108839.2031\n",
      "Epoch [340/500], Train Loss: 108468.2422, Val Loss: 108835.6484\n",
      "Epoch [341/500], Train Loss: 108495.1797, Val Loss: 108832.1797\n",
      "Epoch [342/500], Train Loss: 108467.7422, Val Loss: 108828.7266\n",
      "Epoch [343/500], Train Loss: 108492.9531, Val Loss: 108825.3359\n",
      "Epoch [344/500], Train Loss: 108508.0703, Val Loss: 108822.0547\n",
      "Epoch [345/500], Train Loss: 108489.7344, Val Loss: 108818.8359\n",
      "Epoch [346/500], Train Loss: 108437.6328, Val Loss: 108815.6484\n",
      "Epoch [347/500], Train Loss: 108439.7500, Val Loss: 108812.5312\n",
      "Epoch [348/500], Train Loss: 108431.3047, Val Loss: 108809.4375\n",
      "Epoch [349/500], Train Loss: 108468.5781, Val Loss: 108806.4062\n",
      "Epoch [350/500], Train Loss: 108491.7578, Val Loss: 108803.4375\n",
      "Epoch [351/500], Train Loss: 108448.4609, Val Loss: 108800.5469\n",
      "Epoch [352/500], Train Loss: 108397.6094, Val Loss: 108797.7109\n",
      "Epoch [353/500], Train Loss: 108412.5781, Val Loss: 108794.9297\n",
      "Epoch [354/500], Train Loss: 108441.7969, Val Loss: 108792.1953\n",
      "Epoch [355/500], Train Loss: 108453.8359, Val Loss: 108789.5234\n",
      "Epoch [356/500], Train Loss: 108459.7500, Val Loss: 108786.8516\n",
      "Epoch [357/500], Train Loss: 108428.8750, Val Loss: 108784.2656\n",
      "Epoch [358/500], Train Loss: 108405.2109, Val Loss: 108781.6719\n",
      "Epoch [359/500], Train Loss: 108432.7734, Val Loss: 108779.1484\n",
      "Epoch [360/500], Train Loss: 108424.6172, Val Loss: 108776.6953\n",
      "Epoch [361/500], Train Loss: 108428.6562, Val Loss: 108774.2422\n",
      "Epoch [362/500], Train Loss: 108444.2266, Val Loss: 108771.8516\n",
      "Epoch [363/500], Train Loss: 108391.9531, Val Loss: 108769.4844\n",
      "Epoch [364/500], Train Loss: 108415.6094, Val Loss: 108767.1562\n",
      "Epoch [365/500], Train Loss: 108383.0859, Val Loss: 108764.8672\n",
      "Epoch [366/500], Train Loss: 108431.8828, Val Loss: 108762.6328\n",
      "Epoch [367/500], Train Loss: 108410.3984, Val Loss: 108760.4922\n",
      "Epoch [368/500], Train Loss: 108429.2109, Val Loss: 108758.3438\n",
      "Epoch [369/500], Train Loss: 108376.4531, Val Loss: 108756.2812\n",
      "Epoch [370/500], Train Loss: 108418.3359, Val Loss: 108754.2734\n",
      "Epoch [371/500], Train Loss: 108414.2891, Val Loss: 108752.2344\n",
      "Epoch [372/500], Train Loss: 108372.5312, Val Loss: 108750.2812\n",
      "Epoch [373/500], Train Loss: 108388.0078, Val Loss: 108748.3672\n",
      "Epoch [374/500], Train Loss: 108403.3828, Val Loss: 108746.4609\n",
      "Epoch [375/500], Train Loss: 108372.9375, Val Loss: 108744.5859\n",
      "Epoch [376/500], Train Loss: 108386.9062, Val Loss: 108742.7578\n",
      "Epoch [377/500], Train Loss: 108398.6016, Val Loss: 108740.9453\n",
      "Epoch [378/500], Train Loss: 108383.1094, Val Loss: 108739.1719\n",
      "Epoch [379/500], Train Loss: 108323.0781, Val Loss: 108737.4141\n",
      "Epoch [380/500], Train Loss: 108337.6797, Val Loss: 108735.6797\n",
      "Epoch [381/500], Train Loss: 108361.1406, Val Loss: 108733.9766\n",
      "Epoch [382/500], Train Loss: 108364.1406, Val Loss: 108732.3359\n",
      "Epoch [383/500], Train Loss: 108373.5312, Val Loss: 108730.7188\n",
      "Epoch [384/500], Train Loss: 108378.1094, Val Loss: 108729.1250\n",
      "Epoch [385/500], Train Loss: 108333.1250, Val Loss: 108727.5781\n",
      "Epoch [386/500], Train Loss: 108321.7578, Val Loss: 108726.0469\n",
      "Epoch [387/500], Train Loss: 108334.7266, Val Loss: 108724.5391\n",
      "Epoch [388/500], Train Loss: 108347.9844, Val Loss: 108723.0859\n",
      "Epoch [389/500], Train Loss: 108370.7344, Val Loss: 108721.6328\n",
      "Epoch [390/500], Train Loss: 108331.3516, Val Loss: 108720.2031\n",
      "Epoch [391/500], Train Loss: 108359.7891, Val Loss: 108718.8047\n",
      "Epoch [392/500], Train Loss: 108375.7812, Val Loss: 108717.4375\n",
      "Epoch [393/500], Train Loss: 108347.0625, Val Loss: 108716.1094\n",
      "Epoch [394/500], Train Loss: 108321.9531, Val Loss: 108714.7812\n",
      "Epoch [395/500], Train Loss: 108318.8281, Val Loss: 108713.5234\n",
      "Epoch [396/500], Train Loss: 108356.9219, Val Loss: 108712.2656\n",
      "Epoch [397/500], Train Loss: 108329.3672, Val Loss: 108711.0312\n",
      "Epoch [398/500], Train Loss: 108319.1484, Val Loss: 108709.8281\n",
      "Epoch [399/500], Train Loss: 108291.6953, Val Loss: 108708.6484\n",
      "Epoch [400/500], Train Loss: 108310.9141, Val Loss: 108707.4688\n",
      "Epoch [401/500], Train Loss: 108311.3047, Val Loss: 108706.2969\n",
      "Epoch [402/500], Train Loss: 108308.5234, Val Loss: 108705.1719\n",
      "Epoch [403/500], Train Loss: 108325.7891, Val Loss: 108704.0859\n",
      "Epoch [404/500], Train Loss: 108316.4766, Val Loss: 108703.0391\n",
      "Epoch [405/500], Train Loss: 108311.4375, Val Loss: 108701.9688\n",
      "Epoch [406/500], Train Loss: 108339.6641, Val Loss: 108700.9219\n",
      "Epoch [407/500], Train Loss: 108339.2734, Val Loss: 108699.9297\n",
      "Epoch [408/500], Train Loss: 108292.5312, Val Loss: 108698.9453\n",
      "Epoch [409/500], Train Loss: 108315.4453, Val Loss: 108697.9453\n",
      "Epoch [410/500], Train Loss: 108280.5781, Val Loss: 108696.9922\n",
      "Epoch [411/500], Train Loss: 108306.6719, Val Loss: 108696.0938\n",
      "Epoch [412/500], Train Loss: 108305.4531, Val Loss: 108695.1641\n",
      "Epoch [413/500], Train Loss: 108272.2109, Val Loss: 108694.3047\n",
      "Epoch [414/500], Train Loss: 108292.0625, Val Loss: 108693.4766\n",
      "Epoch [415/500], Train Loss: 108332.7500, Val Loss: 108692.6016\n",
      "Epoch [416/500], Train Loss: 108312.3984, Val Loss: 108691.7266\n",
      "Epoch [417/500], Train Loss: 108332.9453, Val Loss: 108690.9375\n",
      "Epoch [418/500], Train Loss: 108290.2969, Val Loss: 108690.1328\n",
      "Epoch [419/500], Train Loss: 108295.5781, Val Loss: 108689.3359\n",
      "Epoch [420/500], Train Loss: 108305.8594, Val Loss: 108688.5781\n",
      "Epoch [421/500], Train Loss: 108315.6094, Val Loss: 108687.8516\n",
      "Epoch [422/500], Train Loss: 108296.5078, Val Loss: 108687.0781\n",
      "Epoch [423/500], Train Loss: 108301.6250, Val Loss: 108686.3750\n",
      "Epoch [424/500], Train Loss: 108287.4375, Val Loss: 108685.7344\n",
      "Epoch [425/500], Train Loss: 108323.0000, Val Loss: 108685.1172\n",
      "Epoch [426/500], Train Loss: 108267.8750, Val Loss: 108684.4688\n",
      "Epoch [427/500], Train Loss: 108300.0312, Val Loss: 108683.8125\n",
      "Epoch [428/500], Train Loss: 108282.7031, Val Loss: 108683.1719\n",
      "Epoch [429/500], Train Loss: 108278.6250, Val Loss: 108682.5000\n",
      "Epoch [430/500], Train Loss: 108297.8984, Val Loss: 108681.8516\n",
      "Epoch [431/500], Train Loss: 108269.4766, Val Loss: 108681.2344\n",
      "Epoch [432/500], Train Loss: 108285.1250, Val Loss: 108680.6406\n",
      "Epoch [433/500], Train Loss: 108299.9844, Val Loss: 108680.0312\n",
      "Epoch [434/500], Train Loss: 108232.3203, Val Loss: 108679.4609\n",
      "Epoch [435/500], Train Loss: 108300.4844, Val Loss: 108678.8906\n",
      "Epoch [436/500], Train Loss: 108288.3828, Val Loss: 108678.3281\n",
      "Epoch [437/500], Train Loss: 108247.2500, Val Loss: 108677.7969\n",
      "Epoch [438/500], Train Loss: 108285.6484, Val Loss: 108677.3125\n",
      "Epoch [439/500], Train Loss: 108264.7734, Val Loss: 108676.8281\n",
      "Epoch [440/500], Train Loss: 108292.1250, Val Loss: 108676.3750\n",
      "Epoch [441/500], Train Loss: 108207.3984, Val Loss: 108675.8906\n",
      "Epoch [442/500], Train Loss: 108275.7422, Val Loss: 108675.4609\n",
      "Epoch [443/500], Train Loss: 108286.4141, Val Loss: 108675.0391\n",
      "Epoch [444/500], Train Loss: 108274.7188, Val Loss: 108674.5938\n",
      "Epoch [445/500], Train Loss: 108289.8984, Val Loss: 108674.1875\n",
      "Epoch [446/500], Train Loss: 108275.4766, Val Loss: 108673.7656\n",
      "Epoch [447/500], Train Loss: 108265.7266, Val Loss: 108673.2969\n",
      "Epoch [448/500], Train Loss: 108284.1719, Val Loss: 108672.8516\n",
      "Epoch [449/500], Train Loss: 108257.9609, Val Loss: 108672.4688\n",
      "Epoch [450/500], Train Loss: 108273.7500, Val Loss: 108672.1016\n",
      "Epoch [451/500], Train Loss: 108277.7656, Val Loss: 108671.7188\n",
      "Epoch [452/500], Train Loss: 108270.0469, Val Loss: 108671.3281\n",
      "Epoch [453/500], Train Loss: 108265.2109, Val Loss: 108670.9375\n",
      "Epoch [454/500], Train Loss: 108245.9062, Val Loss: 108670.5391\n",
      "Epoch [455/500], Train Loss: 108267.9766, Val Loss: 108670.1172\n",
      "Epoch [456/500], Train Loss: 108271.6641, Val Loss: 108669.7656\n",
      "Epoch [457/500], Train Loss: 108263.5391, Val Loss: 108669.4453\n",
      "Epoch [458/500], Train Loss: 108264.9531, Val Loss: 108669.1250\n",
      "Epoch [459/500], Train Loss: 108246.2109, Val Loss: 108668.8203\n",
      "Epoch [460/500], Train Loss: 108271.0938, Val Loss: 108668.4688\n",
      "Epoch [461/500], Train Loss: 108247.1016, Val Loss: 108668.1328\n",
      "Epoch [462/500], Train Loss: 108241.2188, Val Loss: 108667.8047\n",
      "Epoch [463/500], Train Loss: 108293.2578, Val Loss: 108667.5312\n",
      "Epoch [464/500], Train Loss: 108217.9297, Val Loss: 108667.2656\n",
      "Epoch [465/500], Train Loss: 108288.8125, Val Loss: 108667.0156\n",
      "Epoch [466/500], Train Loss: 108263.3828, Val Loss: 108666.7422\n",
      "Epoch [467/500], Train Loss: 108285.1875, Val Loss: 108666.4297\n",
      "Epoch [468/500], Train Loss: 108266.3906, Val Loss: 108666.1875\n",
      "Epoch [469/500], Train Loss: 108224.6484, Val Loss: 108665.9141\n",
      "Epoch [470/500], Train Loss: 108242.5156, Val Loss: 108665.6875\n",
      "Epoch [471/500], Train Loss: 108242.2422, Val Loss: 108665.4531\n",
      "Epoch [472/500], Train Loss: 108228.5000, Val Loss: 108665.1797\n",
      "Epoch [473/500], Train Loss: 108267.4766, Val Loss: 108664.9766\n",
      "Epoch [474/500], Train Loss: 108260.4062, Val Loss: 108664.6953\n",
      "Epoch [475/500], Train Loss: 108217.2344, Val Loss: 108664.4531\n",
      "Epoch [476/500], Train Loss: 108254.4375, Val Loss: 108664.2734\n",
      "Epoch [477/500], Train Loss: 108247.7422, Val Loss: 108664.0312\n",
      "Epoch [478/500], Train Loss: 108249.6016, Val Loss: 108663.8125\n",
      "Epoch [479/500], Train Loss: 108252.1406, Val Loss: 108663.6484\n",
      "Epoch [480/500], Train Loss: 108242.9062, Val Loss: 108663.4453\n",
      "Epoch [481/500], Train Loss: 108239.2656, Val Loss: 108663.2500\n",
      "Epoch [482/500], Train Loss: 108247.6250, Val Loss: 108663.0703\n",
      "Epoch [483/500], Train Loss: 108220.9453, Val Loss: 108662.8750\n",
      "Epoch [484/500], Train Loss: 108248.6328, Val Loss: 108662.6875\n",
      "Epoch [485/500], Train Loss: 108252.8047, Val Loss: 108662.5391\n",
      "Epoch [486/500], Train Loss: 108256.0938, Val Loss: 108662.3906\n",
      "Epoch [487/500], Train Loss: 108242.2969, Val Loss: 108662.2500\n",
      "Epoch [488/500], Train Loss: 108245.2812, Val Loss: 108662.1328\n",
      "Epoch [489/500], Train Loss: 108252.0312, Val Loss: 108661.9922\n",
      "Epoch [490/500], Train Loss: 108253.8516, Val Loss: 108661.8906\n",
      "Epoch [491/500], Train Loss: 108243.6172, Val Loss: 108661.7812\n",
      "Epoch [492/500], Train Loss: 108251.2969, Val Loss: 108661.6875\n",
      "Epoch [493/500], Train Loss: 108233.4219, Val Loss: 108661.5781\n",
      "Epoch [494/500], Train Loss: 108211.6094, Val Loss: 108661.3906\n",
      "Epoch [495/500], Train Loss: 108241.2344, Val Loss: 108661.2188\n",
      "Epoch [496/500], Train Loss: 108264.7344, Val Loss: 108661.0469\n",
      "Epoch [497/500], Train Loss: 108227.3750, Val Loss: 108660.8594\n",
      "Epoch [498/500], Train Loss: 108232.3828, Val Loss: 108660.7109\n",
      "Epoch [499/500], Train Loss: 108239.9688, Val Loss: 108660.5781\n",
      "Epoch [500/500], Train Loss: 108229.6250, Val Loss: 108660.3906\n",
      "Test Loss: 107630.4688\n",
      "Epoch [1/500], Train Loss: 135358.8750, Val Loss: 135953.3281\n",
      "Epoch [2/500], Train Loss: 135358.6562, Val Loss: 135953.0938\n",
      "Epoch [3/500], Train Loss: 135358.4219, Val Loss: 135952.8438\n",
      "Epoch [4/500], Train Loss: 135358.1719, Val Loss: 135952.6094\n",
      "Epoch [5/500], Train Loss: 135357.9375, Val Loss: 135952.3594\n",
      "Epoch [6/500], Train Loss: 135357.6875, Val Loss: 135952.1250\n",
      "Epoch [7/500], Train Loss: 135357.4531, Val Loss: 135951.8750\n",
      "Epoch [8/500], Train Loss: 135357.2031, Val Loss: 135951.6094\n",
      "Epoch [9/500], Train Loss: 135356.9219, Val Loss: 135951.3438\n",
      "Epoch [10/500], Train Loss: 135356.6875, Val Loss: 135951.0781\n",
      "Epoch [11/500], Train Loss: 135356.4062, Val Loss: 135950.8125\n",
      "Epoch [12/500], Train Loss: 135356.1094, Val Loss: 135950.5000\n",
      "Epoch [13/500], Train Loss: 135355.8281, Val Loss: 135950.2031\n",
      "Epoch [14/500], Train Loss: 135355.5312, Val Loss: 135949.8906\n",
      "Epoch [15/500], Train Loss: 135355.2344, Val Loss: 135949.5469\n",
      "Epoch [16/500], Train Loss: 135354.8906, Val Loss: 135949.2188\n",
      "Epoch [17/500], Train Loss: 135354.5312, Val Loss: 135948.8750\n",
      "Epoch [18/500], Train Loss: 135354.1719, Val Loss: 135948.4688\n",
      "Epoch [19/500], Train Loss: 135353.7812, Val Loss: 135948.0469\n",
      "Epoch [20/500], Train Loss: 135353.3594, Val Loss: 135947.5938\n",
      "Epoch [21/500], Train Loss: 135352.9219, Val Loss: 135947.1250\n",
      "Epoch [22/500], Train Loss: 135352.4219, Val Loss: 135946.5938\n",
      "Epoch [23/500], Train Loss: 135351.9219, Val Loss: 135946.0469\n",
      "Epoch [24/500], Train Loss: 135351.3594, Val Loss: 135945.4375\n",
      "Epoch [25/500], Train Loss: 135350.7656, Val Loss: 135944.7969\n",
      "Epoch [26/500], Train Loss: 135350.1250, Val Loss: 135944.0938\n",
      "Epoch [27/500], Train Loss: 135349.4375, Val Loss: 135943.3594\n",
      "Epoch [28/500], Train Loss: 135348.6875, Val Loss: 135942.5312\n",
      "Epoch [29/500], Train Loss: 135347.8594, Val Loss: 135941.6562\n",
      "Epoch [30/500], Train Loss: 135346.9844, Val Loss: 135940.6562\n",
      "Epoch [31/500], Train Loss: 135346.0156, Val Loss: 135939.6094\n",
      "Epoch [32/500], Train Loss: 135344.9844, Val Loss: 135938.4531\n",
      "Epoch [33/500], Train Loss: 135343.8281, Val Loss: 135937.1875\n",
      "Epoch [34/500], Train Loss: 135342.6094, Val Loss: 135935.8438\n",
      "Epoch [35/500], Train Loss: 135341.2969, Val Loss: 135934.3750\n",
      "Epoch [36/500], Train Loss: 135339.8594, Val Loss: 135932.7812\n",
      "Epoch [37/500], Train Loss: 135338.3125, Val Loss: 135931.0781\n",
      "Epoch [38/500], Train Loss: 135336.6094, Val Loss: 135929.2188\n",
      "Epoch [39/500], Train Loss: 135334.7656, Val Loss: 135927.2188\n",
      "Epoch [40/500], Train Loss: 135332.7500, Val Loss: 135925.0625\n",
      "Epoch [41/500], Train Loss: 135330.6562, Val Loss: 135922.7344\n",
      "Epoch [42/500], Train Loss: 135328.3750, Val Loss: 135920.2500\n",
      "Epoch [43/500], Train Loss: 135325.8906, Val Loss: 135917.5469\n",
      "Epoch [44/500], Train Loss: 135323.2344, Val Loss: 135914.6406\n",
      "Epoch [45/500], Train Loss: 135320.3750, Val Loss: 135911.5000\n",
      "Epoch [46/500], Train Loss: 135317.3281, Val Loss: 135908.1406\n",
      "Epoch [47/500], Train Loss: 135313.9219, Val Loss: 135904.5625\n",
      "Epoch [48/500], Train Loss: 135310.3594, Val Loss: 135900.6719\n",
      "Epoch [49/500], Train Loss: 135306.6094, Val Loss: 135896.5156\n",
      "Epoch [50/500], Train Loss: 135302.4375, Val Loss: 135892.0469\n",
      "Epoch [51/500], Train Loss: 135298.1250, Val Loss: 135887.2812\n",
      "Epoch [52/500], Train Loss: 135293.2812, Val Loss: 135882.1406\n",
      "Epoch [53/500], Train Loss: 135288.2812, Val Loss: 135876.6250\n",
      "Epoch [54/500], Train Loss: 135282.8438, Val Loss: 135870.7344\n",
      "Epoch [55/500], Train Loss: 135276.9844, Val Loss: 135864.4375\n",
      "Epoch [56/500], Train Loss: 135270.7344, Val Loss: 135857.7031\n",
      "Epoch [57/500], Train Loss: 135264.1562, Val Loss: 135850.5000\n",
      "Epoch [58/500], Train Loss: 135256.9062, Val Loss: 135842.7969\n",
      "Epoch [59/500], Train Loss: 135249.4375, Val Loss: 135834.5781\n",
      "Epoch [60/500], Train Loss: 135241.2500, Val Loss: 135825.7812\n",
      "Epoch [61/500], Train Loss: 135232.4375, Val Loss: 135816.4062\n",
      "Epoch [62/500], Train Loss: 135223.2344, Val Loss: 135806.4375\n",
      "Epoch [63/500], Train Loss: 135213.4375, Val Loss: 135795.8125\n",
      "Epoch [64/500], Train Loss: 135203.0312, Val Loss: 135784.5000\n",
      "Epoch [65/500], Train Loss: 135191.7656, Val Loss: 135772.4844\n",
      "Epoch [66/500], Train Loss: 135179.9062, Val Loss: 135759.7188\n",
      "Epoch [67/500], Train Loss: 135167.2500, Val Loss: 135746.1094\n",
      "Epoch [68/500], Train Loss: 135153.6562, Val Loss: 135731.6406\n",
      "Epoch [69/500], Train Loss: 135139.3281, Val Loss: 135716.2656\n",
      "Epoch [70/500], Train Loss: 135124.4844, Val Loss: 135699.9531\n",
      "Epoch [71/500], Train Loss: 135108.0312, Val Loss: 135682.6719\n",
      "Epoch [72/500], Train Loss: 135090.9062, Val Loss: 135664.3438\n",
      "Epoch [73/500], Train Loss: 135073.2969, Val Loss: 135644.9844\n",
      "Epoch [74/500], Train Loss: 135053.8594, Val Loss: 135624.4844\n",
      "Epoch [75/500], Train Loss: 135033.6875, Val Loss: 135602.8438\n",
      "Epoch [76/500], Train Loss: 135012.4375, Val Loss: 135579.9688\n",
      "Epoch [77/500], Train Loss: 134989.5469, Val Loss: 135555.8438\n",
      "Epoch [78/500], Train Loss: 134965.8438, Val Loss: 135530.4219\n",
      "Epoch [79/500], Train Loss: 134940.3906, Val Loss: 135503.6094\n",
      "Epoch [80/500], Train Loss: 134913.9219, Val Loss: 135475.3594\n",
      "Epoch [81/500], Train Loss: 134886.3438, Val Loss: 135445.6250\n",
      "Epoch [82/500], Train Loss: 134856.8906, Val Loss: 135414.3750\n",
      "Epoch [83/500], Train Loss: 134825.8750, Val Loss: 135381.5000\n",
      "Epoch [84/500], Train Loss: 134793.6406, Val Loss: 135347.0000\n",
      "Epoch [85/500], Train Loss: 134759.4375, Val Loss: 135310.7656\n",
      "Epoch [86/500], Train Loss: 134724.1875, Val Loss: 135272.7656\n",
      "Epoch [87/500], Train Loss: 134685.7656, Val Loss: 135232.9531\n",
      "Epoch [88/500], Train Loss: 134647.1406, Val Loss: 135191.2344\n",
      "Epoch [89/500], Train Loss: 134606.2500, Val Loss: 135147.5625\n",
      "Epoch [90/500], Train Loss: 134562.9375, Val Loss: 135101.8906\n",
      "Epoch [91/500], Train Loss: 134517.1562, Val Loss: 135054.1406\n",
      "Epoch [92/500], Train Loss: 134470.5469, Val Loss: 135004.2500\n",
      "Epoch [93/500], Train Loss: 134420.3438, Val Loss: 134952.1406\n",
      "Epoch [94/500], Train Loss: 134369.8281, Val Loss: 134897.7812\n",
      "Epoch [95/500], Train Loss: 134314.5156, Val Loss: 134841.1250\n",
      "Epoch [96/500], Train Loss: 134259.9844, Val Loss: 134782.0625\n",
      "Epoch [97/500], Train Loss: 134201.8438, Val Loss: 134720.5312\n",
      "Epoch [98/500], Train Loss: 134139.2031, Val Loss: 134656.5000\n",
      "Epoch [99/500], Train Loss: 134077.9688, Val Loss: 134589.9062\n",
      "Epoch [100/500], Train Loss: 134011.4688, Val Loss: 134520.7031\n",
      "Epoch [101/500], Train Loss: 133943.8281, Val Loss: 134448.7656\n",
      "Epoch [102/500], Train Loss: 133869.6719, Val Loss: 134374.0938\n",
      "Epoch [103/500], Train Loss: 133798.8281, Val Loss: 134296.6250\n",
      "Epoch [104/500], Train Loss: 133720.0469, Val Loss: 134216.2969\n",
      "Epoch [105/500], Train Loss: 133642.0000, Val Loss: 134133.0156\n",
      "Epoch [106/500], Train Loss: 133557.8906, Val Loss: 134046.7812\n",
      "Epoch [107/500], Train Loss: 133474.5469, Val Loss: 133957.5312\n",
      "Epoch [108/500], Train Loss: 133385.6719, Val Loss: 133865.1562\n",
      "Epoch [109/500], Train Loss: 133293.0938, Val Loss: 133769.7031\n",
      "Epoch [110/500], Train Loss: 133202.9531, Val Loss: 133671.0781\n",
      "Epoch [111/500], Train Loss: 133103.5938, Val Loss: 133569.2188\n",
      "Epoch [112/500], Train Loss: 133003.2188, Val Loss: 133464.1094\n",
      "Epoch [113/500], Train Loss: 132897.2344, Val Loss: 133355.6875\n",
      "Epoch [114/500], Train Loss: 132788.7188, Val Loss: 133243.9219\n",
      "Epoch [115/500], Train Loss: 132682.1094, Val Loss: 133128.7969\n",
      "Epoch [116/500], Train Loss: 132566.3438, Val Loss: 133010.2344\n",
      "Epoch [117/500], Train Loss: 132449.6875, Val Loss: 132888.2344\n",
      "Epoch [118/500], Train Loss: 132329.5938, Val Loss: 132762.7656\n",
      "Epoch [119/500], Train Loss: 132205.3750, Val Loss: 132633.7812\n",
      "Epoch [120/500], Train Loss: 132080.1719, Val Loss: 132501.2656\n",
      "Epoch [121/500], Train Loss: 131946.1250, Val Loss: 132365.1875\n",
      "Epoch [122/500], Train Loss: 131807.2344, Val Loss: 132225.5781\n",
      "Epoch [123/500], Train Loss: 131667.3906, Val Loss: 132082.3594\n",
      "Epoch [124/500], Train Loss: 131531.5625, Val Loss: 131935.5781\n",
      "Epoch [125/500], Train Loss: 131383.5938, Val Loss: 131785.2188\n",
      "Epoch [126/500], Train Loss: 131235.1250, Val Loss: 131631.2812\n",
      "Epoch [127/500], Train Loss: 131080.1094, Val Loss: 131473.7188\n",
      "Epoch [128/500], Train Loss: 130926.5703, Val Loss: 131312.6250\n",
      "Epoch [129/500], Train Loss: 130768.5312, Val Loss: 131148.0000\n",
      "Epoch [130/500], Train Loss: 130601.2422, Val Loss: 130979.8750\n",
      "Epoch [131/500], Train Loss: 130437.3828, Val Loss: 130808.2656\n",
      "Epoch [132/500], Train Loss: 130266.2344, Val Loss: 130633.2188\n",
      "Epoch [133/500], Train Loss: 130093.3047, Val Loss: 130454.8047\n",
      "Epoch [134/500], Train Loss: 129913.3125, Val Loss: 130273.0391\n",
      "Epoch [135/500], Train Loss: 129731.2188, Val Loss: 130087.9922\n",
      "Epoch [136/500], Train Loss: 129550.2500, Val Loss: 129899.7500\n",
      "Epoch [137/500], Train Loss: 129365.9688, Val Loss: 129708.3984\n",
      "Epoch [138/500], Train Loss: 129178.6875, Val Loss: 129514.0312\n",
      "Epoch [139/500], Train Loss: 128978.3203, Val Loss: 129316.7109\n",
      "Epoch [140/500], Train Loss: 128785.5234, Val Loss: 129116.5547\n",
      "Epoch [141/500], Train Loss: 128583.5625, Val Loss: 128913.6328\n",
      "Epoch [142/500], Train Loss: 128373.5078, Val Loss: 128708.1016\n",
      "Epoch [143/500], Train Loss: 128177.5469, Val Loss: 128500.0547\n",
      "Epoch [144/500], Train Loss: 127963.7266, Val Loss: 128289.6016\n",
      "Epoch [145/500], Train Loss: 127759.0391, Val Loss: 128076.9062\n",
      "Epoch [146/500], Train Loss: 127553.7109, Val Loss: 127862.0703\n",
      "Epoch [147/500], Train Loss: 127343.4844, Val Loss: 127645.2578\n",
      "Epoch [148/500], Train Loss: 127116.6562, Val Loss: 127426.5938\n",
      "Epoch [149/500], Train Loss: 126897.8750, Val Loss: 127206.2031\n",
      "Epoch [150/500], Train Loss: 126679.3594, Val Loss: 126984.2578\n",
      "Epoch [151/500], Train Loss: 126452.7188, Val Loss: 126760.9219\n",
      "Epoch [152/500], Train Loss: 126235.0469, Val Loss: 126536.3281\n",
      "Epoch [153/500], Train Loss: 126015.9609, Val Loss: 126310.7031\n",
      "Epoch [154/500], Train Loss: 125780.6484, Val Loss: 126084.1953\n",
      "Epoch [155/500], Train Loss: 125576.4297, Val Loss: 125857.0625\n",
      "Epoch [156/500], Train Loss: 125347.7734, Val Loss: 125629.4844\n",
      "Epoch [157/500], Train Loss: 125108.8750, Val Loss: 125401.6094\n",
      "Epoch [158/500], Train Loss: 124875.5078, Val Loss: 125173.7031\n",
      "Epoch [159/500], Train Loss: 124650.1406, Val Loss: 124945.9609\n",
      "Epoch [160/500], Train Loss: 124425.3750, Val Loss: 124718.5938\n",
      "Epoch [161/500], Train Loss: 124207.1016, Val Loss: 124491.8594\n",
      "Epoch [162/500], Train Loss: 123981.7891, Val Loss: 124265.9375\n",
      "Epoch [163/500], Train Loss: 123739.3594, Val Loss: 124041.0781\n",
      "Epoch [164/500], Train Loss: 123523.5469, Val Loss: 123817.4844\n",
      "Epoch [165/500], Train Loss: 123293.0859, Val Loss: 123595.3594\n",
      "Epoch [166/500], Train Loss: 123074.1484, Val Loss: 123374.9062\n",
      "Epoch [167/500], Train Loss: 122853.4922, Val Loss: 123156.3359\n",
      "Epoch [168/500], Train Loss: 122637.1797, Val Loss: 122939.8203\n",
      "Epoch [169/500], Train Loss: 122419.3750, Val Loss: 122725.4922\n",
      "Epoch [170/500], Train Loss: 122206.2969, Val Loss: 122513.5859\n",
      "Epoch [171/500], Train Loss: 122001.1719, Val Loss: 122304.2109\n",
      "Epoch [172/500], Train Loss: 121790.8359, Val Loss: 122097.5000\n",
      "Epoch [173/500], Train Loss: 121573.6875, Val Loss: 121893.6094\n",
      "Epoch [174/500], Train Loss: 121359.5234, Val Loss: 121692.5469\n",
      "Epoch [175/500], Train Loss: 121169.9062, Val Loss: 121494.4922\n",
      "Epoch [176/500], Train Loss: 120997.0391, Val Loss: 121299.5234\n",
      "Epoch [177/500], Train Loss: 120778.4219, Val Loss: 121107.6641\n",
      "Epoch [178/500], Train Loss: 120584.0938, Val Loss: 120918.9844\n",
      "Epoch [179/500], Train Loss: 120396.4688, Val Loss: 120733.5234\n",
      "Epoch [180/500], Train Loss: 120224.5859, Val Loss: 120551.3047\n",
      "Epoch [181/500], Train Loss: 120040.9062, Val Loss: 120372.3438\n",
      "Epoch [182/500], Train Loss: 119876.1328, Val Loss: 120196.6250\n",
      "Epoch [183/500], Train Loss: 119678.4531, Val Loss: 120024.1016\n",
      "Epoch [184/500], Train Loss: 119522.4219, Val Loss: 119854.7500\n",
      "Epoch [185/500], Train Loss: 119361.6641, Val Loss: 119688.5469\n",
      "Epoch [186/500], Train Loss: 119189.2969, Val Loss: 119525.4062\n",
      "Epoch [187/500], Train Loss: 119026.5469, Val Loss: 119365.2422\n",
      "Epoch [188/500], Train Loss: 118850.9688, Val Loss: 119207.9688\n",
      "Epoch [189/500], Train Loss: 118703.3672, Val Loss: 119053.5469\n",
      "Epoch [190/500], Train Loss: 118556.1484, Val Loss: 118901.9219\n",
      "Epoch [191/500], Train Loss: 118423.4766, Val Loss: 118752.9141\n",
      "Epoch [192/500], Train Loss: 118233.7891, Val Loss: 118606.5312\n",
      "Epoch [193/500], Train Loss: 118099.0625, Val Loss: 118462.6406\n",
      "Epoch [194/500], Train Loss: 117965.7656, Val Loss: 118321.1484\n",
      "Epoch [195/500], Train Loss: 117828.0234, Val Loss: 118182.0625\n",
      "Epoch [196/500], Train Loss: 117696.8594, Val Loss: 118045.2422\n",
      "Epoch [197/500], Train Loss: 117532.6797, Val Loss: 117910.6250\n",
      "Epoch [198/500], Train Loss: 117414.4375, Val Loss: 117778.1328\n",
      "Epoch [199/500], Train Loss: 117298.5703, Val Loss: 117647.7422\n",
      "Epoch [200/500], Train Loss: 117158.1094, Val Loss: 117519.3828\n",
      "Epoch [201/500], Train Loss: 117014.0547, Val Loss: 117392.9062\n",
      "Epoch [202/500], Train Loss: 116911.9922, Val Loss: 117268.3203\n",
      "Epoch [203/500], Train Loss: 116774.1953, Val Loss: 117145.5781\n",
      "Epoch [204/500], Train Loss: 116686.0859, Val Loss: 117024.6328\n",
      "Epoch [205/500], Train Loss: 116572.2500, Val Loss: 116905.4453\n",
      "Epoch [206/500], Train Loss: 116443.3672, Val Loss: 116787.9766\n",
      "Epoch [207/500], Train Loss: 116291.9688, Val Loss: 116672.1719\n",
      "Epoch [208/500], Train Loss: 116207.2734, Val Loss: 116557.9766\n",
      "Epoch [209/500], Train Loss: 116070.1875, Val Loss: 116445.3984\n",
      "Epoch [210/500], Train Loss: 115998.9609, Val Loss: 116334.3828\n",
      "Epoch [211/500], Train Loss: 115870.7734, Val Loss: 116224.9297\n",
      "Epoch [212/500], Train Loss: 115772.6016, Val Loss: 116116.9844\n",
      "Epoch [213/500], Train Loss: 115663.7578, Val Loss: 116010.5312\n",
      "Epoch [214/500], Train Loss: 115557.8125, Val Loss: 115905.6016\n",
      "Epoch [215/500], Train Loss: 115473.2500, Val Loss: 115802.1016\n",
      "Epoch [216/500], Train Loss: 115353.3516, Val Loss: 115700.0625\n",
      "Epoch [217/500], Train Loss: 115272.2734, Val Loss: 115599.4609\n",
      "Epoch [218/500], Train Loss: 115150.9453, Val Loss: 115500.3047\n",
      "Epoch [219/500], Train Loss: 115076.1016, Val Loss: 115402.5391\n",
      "Epoch [220/500], Train Loss: 114992.7891, Val Loss: 115306.2109\n",
      "Epoch [221/500], Train Loss: 114879.7578, Val Loss: 115211.2422\n",
      "Epoch [222/500], Train Loss: 114809.8672, Val Loss: 115117.6562\n",
      "Epoch [223/500], Train Loss: 114716.8594, Val Loss: 115025.4297\n",
      "Epoch [224/500], Train Loss: 114605.5156, Val Loss: 114934.5391\n",
      "Epoch [225/500], Train Loss: 114527.2500, Val Loss: 114844.9844\n",
      "Epoch [226/500], Train Loss: 114453.6250, Val Loss: 114756.8125\n",
      "Epoch [227/500], Train Loss: 114325.8594, Val Loss: 114669.8438\n",
      "Epoch [228/500], Train Loss: 114292.0391, Val Loss: 114584.2656\n",
      "Epoch [229/500], Train Loss: 114182.0938, Val Loss: 114499.9375\n",
      "Epoch [230/500], Train Loss: 114106.5547, Val Loss: 114416.8594\n",
      "Epoch [231/500], Train Loss: 114036.5234, Val Loss: 114335.1016\n",
      "Epoch [232/500], Train Loss: 113927.9922, Val Loss: 114254.5547\n",
      "Epoch [233/500], Train Loss: 113869.8672, Val Loss: 114175.2578\n",
      "Epoch [234/500], Train Loss: 113815.3750, Val Loss: 114097.2109\n",
      "Epoch [235/500], Train Loss: 113738.3359, Val Loss: 114020.3672\n",
      "Epoch [236/500], Train Loss: 113648.8125, Val Loss: 113944.7578\n",
      "Epoch [237/500], Train Loss: 113589.0234, Val Loss: 113870.3125\n",
      "Epoch [238/500], Train Loss: 113488.2031, Val Loss: 113797.0781\n",
      "Epoch [239/500], Train Loss: 113418.6562, Val Loss: 113724.9531\n",
      "Epoch [240/500], Train Loss: 113367.3047, Val Loss: 113654.0078\n",
      "Epoch [241/500], Train Loss: 113315.7188, Val Loss: 113584.2109\n",
      "Epoch [242/500], Train Loss: 113241.0625, Val Loss: 113515.5156\n",
      "Epoch [243/500], Train Loss: 113175.6641, Val Loss: 113447.9375\n",
      "Epoch [244/500], Train Loss: 113094.3516, Val Loss: 113381.4922\n",
      "Epoch [245/500], Train Loss: 113027.2734, Val Loss: 113316.1562\n",
      "Epoch [246/500], Train Loss: 112983.1953, Val Loss: 113251.8594\n",
      "Epoch [247/500], Train Loss: 112883.2031, Val Loss: 113188.6484\n",
      "Epoch [248/500], Train Loss: 112863.9453, Val Loss: 113126.4844\n",
      "Epoch [249/500], Train Loss: 112804.4609, Val Loss: 113065.3672\n",
      "Epoch [250/500], Train Loss: 112743.6875, Val Loss: 113005.2656\n",
      "Epoch [251/500], Train Loss: 112655.9688, Val Loss: 112946.1641\n",
      "Epoch [252/500], Train Loss: 112619.1016, Val Loss: 112888.0469\n",
      "Epoch [253/500], Train Loss: 112588.2656, Val Loss: 112830.9219\n",
      "Epoch [254/500], Train Loss: 112497.3125, Val Loss: 112774.7578\n",
      "Epoch [255/500], Train Loss: 112458.7734, Val Loss: 112719.5469\n",
      "Epoch [256/500], Train Loss: 112403.1016, Val Loss: 112665.2656\n",
      "Epoch [257/500], Train Loss: 112387.4219, Val Loss: 112611.9375\n",
      "Epoch [258/500], Train Loss: 112297.3906, Val Loss: 112559.5156\n",
      "Epoch [259/500], Train Loss: 112233.2266, Val Loss: 112508.0000\n",
      "Epoch [260/500], Train Loss: 112199.6875, Val Loss: 112457.3750\n",
      "Epoch [261/500], Train Loss: 112132.5156, Val Loss: 112407.6094\n",
      "Epoch [262/500], Train Loss: 112121.7500, Val Loss: 112358.7422\n",
      "Epoch [263/500], Train Loss: 112094.5938, Val Loss: 112310.7266\n",
      "Epoch [264/500], Train Loss: 112022.6484, Val Loss: 112263.5469\n",
      "Epoch [265/500], Train Loss: 111977.6562, Val Loss: 112217.2266\n",
      "Epoch [266/500], Train Loss: 111918.0547, Val Loss: 112171.6797\n",
      "Epoch [267/500], Train Loss: 111892.6875, Val Loss: 112126.9062\n",
      "Epoch [268/500], Train Loss: 111854.7266, Val Loss: 112083.0000\n",
      "Epoch [269/500], Train Loss: 111818.6406, Val Loss: 112039.8594\n",
      "Epoch [270/500], Train Loss: 111753.6953, Val Loss: 111997.4453\n",
      "Epoch [271/500], Train Loss: 111721.7031, Val Loss: 111955.8203\n",
      "Epoch [272/500], Train Loss: 111669.1719, Val Loss: 111914.9609\n",
      "Epoch [273/500], Train Loss: 111610.9062, Val Loss: 111874.7891\n",
      "Epoch [274/500], Train Loss: 111617.6172, Val Loss: 111835.3594\n",
      "Epoch [275/500], Train Loss: 111575.3984, Val Loss: 111796.6016\n",
      "Epoch [276/500], Train Loss: 111543.2422, Val Loss: 111758.5469\n",
      "Epoch [277/500], Train Loss: 111489.7031, Val Loss: 111721.1797\n",
      "Epoch [278/500], Train Loss: 111479.7734, Val Loss: 111684.2109\n",
      "Epoch [279/500], Train Loss: 111451.5547, Val Loss: 111647.7656\n",
      "Epoch [280/500], Train Loss: 111390.3516, Val Loss: 111611.9766\n",
      "Epoch [281/500], Train Loss: 111363.6406, Val Loss: 111576.7656\n",
      "Epoch [282/500], Train Loss: 111296.4922, Val Loss: 111542.1328\n",
      "Epoch [283/500], Train Loss: 111315.9297, Val Loss: 111508.1484\n",
      "Epoch [284/500], Train Loss: 111294.8828, Val Loss: 111474.7812\n",
      "Epoch [285/500], Train Loss: 111231.3516, Val Loss: 111441.9453\n",
      "Epoch [286/500], Train Loss: 111172.6328, Val Loss: 111409.6484\n",
      "Epoch [287/500], Train Loss: 111204.6797, Val Loss: 111377.9531\n",
      "Epoch [288/500], Train Loss: 111116.7344, Val Loss: 111346.7969\n",
      "Epoch [289/500], Train Loss: 111117.7031, Val Loss: 111316.1484\n",
      "Epoch [290/500], Train Loss: 111075.5312, Val Loss: 111286.0000\n",
      "Epoch [291/500], Train Loss: 111082.4297, Val Loss: 111256.4453\n",
      "Epoch [292/500], Train Loss: 111038.5859, Val Loss: 111227.3828\n",
      "Epoch [293/500], Train Loss: 110991.2812, Val Loss: 111198.7891\n",
      "Epoch [294/500], Train Loss: 110957.8672, Val Loss: 111170.6953\n",
      "Epoch [295/500], Train Loss: 110923.8750, Val Loss: 111143.0703\n",
      "Epoch [296/500], Train Loss: 110900.1406, Val Loss: 111115.9453\n",
      "Epoch [297/500], Train Loss: 110857.1953, Val Loss: 111089.2266\n",
      "Epoch [298/500], Train Loss: 110856.1719, Val Loss: 111062.8984\n",
      "Epoch [299/500], Train Loss: 110839.4219, Val Loss: 111037.0781\n",
      "Epoch [300/500], Train Loss: 110817.5000, Val Loss: 111011.6797\n",
      "Epoch [301/500], Train Loss: 110789.5312, Val Loss: 110986.6562\n",
      "Epoch [302/500], Train Loss: 110782.0703, Val Loss: 110962.1016\n",
      "Epoch [303/500], Train Loss: 110759.1016, Val Loss: 110937.9375\n",
      "Epoch [304/500], Train Loss: 110706.4688, Val Loss: 110914.1562\n",
      "Epoch [305/500], Train Loss: 110727.8047, Val Loss: 110890.7656\n",
      "Epoch [306/500], Train Loss: 110660.1953, Val Loss: 110867.7266\n",
      "Epoch [307/500], Train Loss: 110633.5000, Val Loss: 110845.0391\n",
      "Epoch [308/500], Train Loss: 110608.0469, Val Loss: 110822.7422\n",
      "Epoch [309/500], Train Loss: 110614.0469, Val Loss: 110800.7812\n",
      "Epoch [310/500], Train Loss: 110572.3516, Val Loss: 110779.1562\n",
      "Epoch [311/500], Train Loss: 110556.8828, Val Loss: 110757.8906\n",
      "Epoch [312/500], Train Loss: 110558.1484, Val Loss: 110736.9531\n",
      "Epoch [313/500], Train Loss: 110481.0625, Val Loss: 110716.2812\n",
      "Epoch [314/500], Train Loss: 110525.0156, Val Loss: 110695.9453\n",
      "Epoch [315/500], Train Loss: 110486.6172, Val Loss: 110675.9219\n",
      "Epoch [316/500], Train Loss: 110475.3281, Val Loss: 110656.1406\n",
      "Epoch [317/500], Train Loss: 110438.5391, Val Loss: 110636.7031\n",
      "Epoch [318/500], Train Loss: 110424.9609, Val Loss: 110617.5156\n",
      "Epoch [319/500], Train Loss: 110407.3984, Val Loss: 110598.6016\n",
      "Epoch [320/500], Train Loss: 110406.2578, Val Loss: 110579.9609\n",
      "Epoch [321/500], Train Loss: 110381.0859, Val Loss: 110561.5547\n",
      "Epoch [322/500], Train Loss: 110361.5156, Val Loss: 110543.4453\n",
      "Epoch [323/500], Train Loss: 110342.0391, Val Loss: 110525.5781\n",
      "Epoch [324/500], Train Loss: 110302.0938, Val Loss: 110507.9531\n",
      "Epoch [325/500], Train Loss: 110332.5625, Val Loss: 110490.5391\n",
      "Epoch [326/500], Train Loss: 110301.8594, Val Loss: 110473.3750\n",
      "Epoch [327/500], Train Loss: 110272.3203, Val Loss: 110456.4453\n",
      "Epoch [328/500], Train Loss: 110284.6641, Val Loss: 110439.6719\n",
      "Epoch [329/500], Train Loss: 110244.6797, Val Loss: 110423.1797\n",
      "Epoch [330/500], Train Loss: 110193.5469, Val Loss: 110406.8984\n",
      "Epoch [331/500], Train Loss: 110212.0469, Val Loss: 110390.7891\n",
      "Epoch [332/500], Train Loss: 110181.5000, Val Loss: 110374.8906\n",
      "Epoch [333/500], Train Loss: 110166.5312, Val Loss: 110359.1719\n",
      "Epoch [334/500], Train Loss: 110162.7500, Val Loss: 110343.6641\n",
      "Epoch [335/500], Train Loss: 110122.2500, Val Loss: 110328.3203\n",
      "Epoch [336/500], Train Loss: 110135.7734, Val Loss: 110313.1875\n",
      "Epoch [337/500], Train Loss: 110123.1484, Val Loss: 110298.2109\n",
      "Epoch [338/500], Train Loss: 110091.0859, Val Loss: 110283.4141\n",
      "Epoch [339/500], Train Loss: 110083.0078, Val Loss: 110268.8125\n",
      "Epoch [340/500], Train Loss: 110067.5391, Val Loss: 110254.3672\n",
      "Epoch [341/500], Train Loss: 110070.6719, Val Loss: 110240.0781\n",
      "Epoch [342/500], Train Loss: 110022.3203, Val Loss: 110225.9453\n",
      "Epoch [343/500], Train Loss: 110015.9688, Val Loss: 110211.9453\n",
      "Epoch [344/500], Train Loss: 110024.0078, Val Loss: 110198.0938\n",
      "Epoch [345/500], Train Loss: 109995.4844, Val Loss: 110184.3984\n",
      "Epoch [346/500], Train Loss: 109947.7109, Val Loss: 110170.8516\n",
      "Epoch [347/500], Train Loss: 109974.5469, Val Loss: 110157.4531\n",
      "Epoch [348/500], Train Loss: 109958.6250, Val Loss: 110144.2188\n",
      "Epoch [349/500], Train Loss: 109922.2891, Val Loss: 110131.0859\n",
      "Epoch [350/500], Train Loss: 109916.2031, Val Loss: 110118.1172\n",
      "Epoch [351/500], Train Loss: 109904.1484, Val Loss: 110105.2656\n",
      "Epoch [352/500], Train Loss: 109864.6094, Val Loss: 110092.5312\n",
      "Epoch [353/500], Train Loss: 109896.3828, Val Loss: 110079.9062\n",
      "Epoch [354/500], Train Loss: 109884.7969, Val Loss: 110067.4688\n",
      "Epoch [355/500], Train Loss: 109857.2734, Val Loss: 110055.1484\n",
      "Epoch [356/500], Train Loss: 109844.0234, Val Loss: 110042.9297\n",
      "Epoch [357/500], Train Loss: 109833.8906, Val Loss: 110030.8203\n",
      "Epoch [358/500], Train Loss: 109826.0078, Val Loss: 110018.8359\n",
      "Epoch [359/500], Train Loss: 109799.0312, Val Loss: 110006.9609\n",
      "Epoch [360/500], Train Loss: 109775.3359, Val Loss: 109995.2344\n",
      "Epoch [361/500], Train Loss: 109788.0000, Val Loss: 109983.6016\n",
      "Epoch [362/500], Train Loss: 109745.9219, Val Loss: 109972.1016\n",
      "Epoch [363/500], Train Loss: 109766.6172, Val Loss: 109960.6797\n",
      "Epoch [364/500], Train Loss: 109745.3906, Val Loss: 109949.3438\n",
      "Epoch [365/500], Train Loss: 109734.2031, Val Loss: 109938.1328\n",
      "Epoch [366/500], Train Loss: 109733.7656, Val Loss: 109927.0234\n",
      "Epoch [367/500], Train Loss: 109709.8906, Val Loss: 109915.9922\n",
      "Epoch [368/500], Train Loss: 109687.9375, Val Loss: 109905.1172\n",
      "Epoch [369/500], Train Loss: 109700.3828, Val Loss: 109894.3359\n",
      "Epoch [370/500], Train Loss: 109701.3281, Val Loss: 109883.5938\n",
      "Epoch [371/500], Train Loss: 109665.3438, Val Loss: 109872.9688\n",
      "Epoch [372/500], Train Loss: 109655.6328, Val Loss: 109862.4219\n",
      "Epoch [373/500], Train Loss: 109627.2891, Val Loss: 109852.0078\n",
      "Epoch [374/500], Train Loss: 109629.1719, Val Loss: 109841.6484\n",
      "Epoch [375/500], Train Loss: 109602.4531, Val Loss: 109831.4219\n",
      "Epoch [376/500], Train Loss: 109607.0391, Val Loss: 109821.2969\n",
      "Epoch [377/500], Train Loss: 109583.3047, Val Loss: 109811.2188\n",
      "Epoch [378/500], Train Loss: 109599.3203, Val Loss: 109801.2266\n",
      "Epoch [379/500], Train Loss: 109573.0703, Val Loss: 109791.2969\n",
      "Epoch [380/500], Train Loss: 109566.4062, Val Loss: 109781.5078\n",
      "Epoch [381/500], Train Loss: 109581.6406, Val Loss: 109771.7812\n",
      "Epoch [382/500], Train Loss: 109551.1094, Val Loss: 109762.1250\n",
      "Epoch [383/500], Train Loss: 109551.3516, Val Loss: 109752.5625\n",
      "Epoch [384/500], Train Loss: 109539.9609, Val Loss: 109743.0859\n",
      "Epoch [385/500], Train Loss: 109477.3594, Val Loss: 109733.6719\n",
      "Epoch [386/500], Train Loss: 109488.3750, Val Loss: 109724.3125\n",
      "Epoch [387/500], Train Loss: 109525.8047, Val Loss: 109715.0781\n",
      "Epoch [388/500], Train Loss: 109488.2422, Val Loss: 109705.9062\n",
      "Epoch [389/500], Train Loss: 109472.1484, Val Loss: 109696.8203\n",
      "Epoch [390/500], Train Loss: 109434.1328, Val Loss: 109687.7969\n",
      "Epoch [391/500], Train Loss: 109471.8672, Val Loss: 109678.8906\n",
      "Epoch [392/500], Train Loss: 109466.0625, Val Loss: 109670.0312\n",
      "Epoch [393/500], Train Loss: 109410.0312, Val Loss: 109661.2344\n",
      "Epoch [394/500], Train Loss: 109428.2891, Val Loss: 109652.5156\n",
      "Epoch [395/500], Train Loss: 109407.5938, Val Loss: 109643.8828\n",
      "Epoch [396/500], Train Loss: 109404.8516, Val Loss: 109635.2969\n",
      "Epoch [397/500], Train Loss: 109436.2734, Val Loss: 109626.7812\n",
      "Epoch [398/500], Train Loss: 109395.1875, Val Loss: 109618.3438\n",
      "Epoch [399/500], Train Loss: 109372.7656, Val Loss: 109609.9609\n",
      "Epoch [400/500], Train Loss: 109360.0312, Val Loss: 109601.6562\n",
      "Epoch [401/500], Train Loss: 109354.0625, Val Loss: 109593.4688\n",
      "Epoch [402/500], Train Loss: 109380.5156, Val Loss: 109585.3125\n",
      "Epoch [403/500], Train Loss: 109346.0703, Val Loss: 109577.2188\n",
      "Epoch [404/500], Train Loss: 109353.9297, Val Loss: 109569.1953\n",
      "Epoch [405/500], Train Loss: 109323.8281, Val Loss: 109561.2109\n",
      "Epoch [406/500], Train Loss: 109365.2344, Val Loss: 109553.3594\n",
      "Epoch [407/500], Train Loss: 109289.8828, Val Loss: 109545.5547\n",
      "Epoch [408/500], Train Loss: 109306.2188, Val Loss: 109537.7656\n",
      "Epoch [409/500], Train Loss: 109291.7109, Val Loss: 109530.0703\n",
      "Epoch [410/500], Train Loss: 109309.8594, Val Loss: 109522.4453\n",
      "Epoch [411/500], Train Loss: 109250.9922, Val Loss: 109514.8516\n",
      "Epoch [412/500], Train Loss: 109280.7188, Val Loss: 109507.3125\n",
      "Epoch [413/500], Train Loss: 109243.8438, Val Loss: 109499.8047\n",
      "Epoch [414/500], Train Loss: 109285.8516, Val Loss: 109492.4141\n",
      "Epoch [415/500], Train Loss: 109262.9531, Val Loss: 109485.0938\n",
      "Epoch [416/500], Train Loss: 109243.8125, Val Loss: 109477.7891\n",
      "Epoch [417/500], Train Loss: 109214.8281, Val Loss: 109470.5312\n",
      "Epoch [418/500], Train Loss: 109225.6250, Val Loss: 109463.3672\n",
      "Epoch [419/500], Train Loss: 109196.1484, Val Loss: 109456.2656\n",
      "Epoch [420/500], Train Loss: 109210.8125, Val Loss: 109449.1875\n",
      "Epoch [421/500], Train Loss: 109171.4453, Val Loss: 109442.1484\n",
      "Epoch [422/500], Train Loss: 109177.2344, Val Loss: 109435.2344\n",
      "Epoch [423/500], Train Loss: 109199.9531, Val Loss: 109428.3359\n",
      "Epoch [424/500], Train Loss: 109139.2422, Val Loss: 109421.4453\n",
      "Epoch [425/500], Train Loss: 109173.9922, Val Loss: 109414.6328\n",
      "Epoch [426/500], Train Loss: 109151.0234, Val Loss: 109407.8750\n",
      "Epoch [427/500], Train Loss: 109171.5469, Val Loss: 109401.1875\n",
      "Epoch [428/500], Train Loss: 109133.2578, Val Loss: 109394.5625\n",
      "Epoch [429/500], Train Loss: 109133.1328, Val Loss: 109387.9531\n",
      "Epoch [430/500], Train Loss: 109133.8828, Val Loss: 109381.4375\n",
      "Epoch [431/500], Train Loss: 109111.3125, Val Loss: 109374.9766\n",
      "Epoch [432/500], Train Loss: 109136.7266, Val Loss: 109368.5000\n",
      "Epoch [433/500], Train Loss: 109129.5938, Val Loss: 109362.1172\n",
      "Epoch [434/500], Train Loss: 109079.0234, Val Loss: 109355.8125\n",
      "Epoch [435/500], Train Loss: 109064.6562, Val Loss: 109349.5391\n",
      "Epoch [436/500], Train Loss: 109098.8906, Val Loss: 109343.3125\n",
      "Epoch [437/500], Train Loss: 109095.4609, Val Loss: 109337.1562\n",
      "Epoch [438/500], Train Loss: 109069.5703, Val Loss: 109331.0078\n",
      "Epoch [439/500], Train Loss: 109081.1250, Val Loss: 109324.9453\n",
      "Epoch [440/500], Train Loss: 109057.5469, Val Loss: 109318.9531\n",
      "Epoch [441/500], Train Loss: 109058.3906, Val Loss: 109313.0000\n",
      "Epoch [442/500], Train Loss: 109082.6953, Val Loss: 109307.0781\n",
      "Epoch [443/500], Train Loss: 109064.7422, Val Loss: 109301.2109\n",
      "Epoch [444/500], Train Loss: 109032.5000, Val Loss: 109295.3906\n",
      "Epoch [445/500], Train Loss: 109025.3359, Val Loss: 109289.5938\n",
      "Epoch [446/500], Train Loss: 109013.8047, Val Loss: 109283.8359\n",
      "Epoch [447/500], Train Loss: 109020.6250, Val Loss: 109278.1719\n",
      "Epoch [448/500], Train Loss: 108988.6328, Val Loss: 109272.5859\n",
      "Epoch [449/500], Train Loss: 108989.1484, Val Loss: 109266.9844\n",
      "Epoch [450/500], Train Loss: 108974.3828, Val Loss: 109261.4531\n",
      "Epoch [451/500], Train Loss: 108982.1016, Val Loss: 109255.9609\n",
      "Epoch [452/500], Train Loss: 108979.6719, Val Loss: 109250.4922\n",
      "Epoch [453/500], Train Loss: 108969.5234, Val Loss: 109245.0469\n",
      "Epoch [454/500], Train Loss: 108942.6172, Val Loss: 109239.6641\n",
      "Epoch [455/500], Train Loss: 108959.3203, Val Loss: 109234.3359\n",
      "Epoch [456/500], Train Loss: 108944.1172, Val Loss: 109229.0469\n",
      "Epoch [457/500], Train Loss: 108952.0703, Val Loss: 109223.7969\n",
      "Epoch [458/500], Train Loss: 108948.6094, Val Loss: 109218.5859\n",
      "Epoch [459/500], Train Loss: 108930.9609, Val Loss: 109213.4375\n",
      "Epoch [460/500], Train Loss: 108941.0547, Val Loss: 109208.3203\n",
      "Epoch [461/500], Train Loss: 108923.5859, Val Loss: 109203.2422\n",
      "Epoch [462/500], Train Loss: 108902.8750, Val Loss: 109198.2188\n",
      "Epoch [463/500], Train Loss: 108889.1484, Val Loss: 109193.2188\n",
      "Epoch [464/500], Train Loss: 108887.1641, Val Loss: 109188.2266\n",
      "Epoch [465/500], Train Loss: 108905.7266, Val Loss: 109183.3125\n",
      "Epoch [466/500], Train Loss: 108889.0156, Val Loss: 109178.4375\n",
      "Epoch [467/500], Train Loss: 108892.0156, Val Loss: 109173.5938\n",
      "Epoch [468/500], Train Loss: 108902.1328, Val Loss: 109168.8047\n",
      "Epoch [469/500], Train Loss: 108868.7891, Val Loss: 109164.0312\n",
      "Epoch [470/500], Train Loss: 108877.7969, Val Loss: 109159.3047\n",
      "Epoch [471/500], Train Loss: 108884.9688, Val Loss: 109154.6328\n",
      "Epoch [472/500], Train Loss: 108856.9297, Val Loss: 109150.0000\n",
      "Epoch [473/500], Train Loss: 108859.0781, Val Loss: 109145.4062\n",
      "Epoch [474/500], Train Loss: 108862.3594, Val Loss: 109140.8594\n",
      "Epoch [475/500], Train Loss: 108876.4609, Val Loss: 109136.3203\n",
      "Epoch [476/500], Train Loss: 108862.7578, Val Loss: 109131.8203\n",
      "Epoch [477/500], Train Loss: 108825.7344, Val Loss: 109127.3672\n",
      "Epoch [478/500], Train Loss: 108831.5625, Val Loss: 109122.9609\n",
      "Epoch [479/500], Train Loss: 108832.5391, Val Loss: 109118.6016\n",
      "Epoch [480/500], Train Loss: 108825.4375, Val Loss: 109114.2656\n",
      "Epoch [481/500], Train Loss: 108828.1328, Val Loss: 109109.9922\n",
      "Epoch [482/500], Train Loss: 108811.1719, Val Loss: 109105.7266\n",
      "Epoch [483/500], Train Loss: 108807.9453, Val Loss: 109101.4766\n",
      "Epoch [484/500], Train Loss: 108803.8047, Val Loss: 109097.2500\n",
      "Epoch [485/500], Train Loss: 108822.5469, Val Loss: 109093.1016\n",
      "Epoch [486/500], Train Loss: 108795.7031, Val Loss: 109089.0078\n",
      "Epoch [487/500], Train Loss: 108763.2422, Val Loss: 109084.8906\n",
      "Epoch [488/500], Train Loss: 108768.4062, Val Loss: 109080.8203\n",
      "Epoch [489/500], Train Loss: 108762.6094, Val Loss: 109076.8125\n",
      "Epoch [490/500], Train Loss: 108793.8672, Val Loss: 109072.8281\n",
      "Epoch [491/500], Train Loss: 108780.4453, Val Loss: 109068.8750\n",
      "Epoch [492/500], Train Loss: 108779.2031, Val Loss: 109064.9766\n",
      "Epoch [493/500], Train Loss: 108763.0000, Val Loss: 109061.1250\n",
      "Epoch [494/500], Train Loss: 108755.1406, Val Loss: 109057.2969\n",
      "Epoch [495/500], Train Loss: 108738.7656, Val Loss: 109053.4844\n",
      "Epoch [496/500], Train Loss: 108757.5938, Val Loss: 109049.6953\n",
      "Epoch [497/500], Train Loss: 108749.8750, Val Loss: 109045.9688\n",
      "Epoch [498/500], Train Loss: 108730.6172, Val Loss: 109042.2578\n",
      "Epoch [499/500], Train Loss: 108756.3984, Val Loss: 109038.5859\n",
      "Epoch [500/500], Train Loss: 108724.2266, Val Loss: 109034.9766\n",
      "Test Loss: 108193.3906\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # Experiment with hidden layer sizes:\n",
    "        self.fc1 = nn.Linear(450, 256)  \n",
    "        self.fc2 = nn.Linear(256, 128)  \n",
    "        self.fc3 = nn.Linear(128, 64)  # Reduced layer sizes\n",
    "        self.fc4 = nn.Linear(64, 60)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)  \n",
    "        self.relu = nn.ReLU()  # ReLU for efficiency\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x) \n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x) \n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Hyperparameter tuning loop\n",
    "learning_rates = [0.001, 0.0007, 0.0005, 0.0003]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = NeuralNetwork()  # New model instance for each learning rate\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(500):  # You can adjust the number of epochs\n",
    "        # Forward pass\n",
    "        model.train()\n",
    "        outputs = model(X_train_tensor)\n",
    "        train_loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor)\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch [{epoch+1}/500], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(\"Early stopping...\")\n",
    "                break\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(torch.tensor(X_test))\n",
    "        test_loss = criterion(test_outputs, torch.tensor(y_test))\n",
    "        print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'further_improved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f1a38-51ec-4b9d-840c-9830a9849b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
