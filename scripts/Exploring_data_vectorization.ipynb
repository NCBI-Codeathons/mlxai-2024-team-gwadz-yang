{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "024db6aa-8107-4e9c-b86c-0c221476d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85341fc0-5c8e-4269-a8ec-f687c5eb5918",
   "metadata": {},
   "source": [
    "### Try TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41077c6-e9ae-41ed-aeed-4685d1577ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 38)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['90', 'also', 'and', 'are', 'as', 'atp', 'atpase', 'atpases',\n",
       "       'belonging', 'binding', 'dna', 'domain', 'domains', 'ghkl',\n",
       "       'gyrase', 'hatpase', 'heat', 'histidine', 'hsp90', 'includes',\n",
       "       'kinase', 'like', 'mismatch', 'mutl', 'of', 'phytochrome',\n",
       "       'protein', 'proteins', 'referred', 'repair', 'several', 'shock',\n",
       "       'such', 'superfamily', 'the', 'this', 'to', 'topoisomerases'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Simulate your data as a dictionary for simplicity\n",
    "data = {\n",
    "    'Domain_ID': 'c100075',\n",
    "    'Short_Name':  'HATPase Superfamily',\n",
    "    'Full_Name': 'Histidine kinase-like ATPase domain',\n",
    "    'Description': 'This superfamily includes the histidine kinase-like ATPase (HATPase) domains of several ATP-binding proteins such as histidine kinase, DNA gyrase B, topoisomerases, heat shock protein 90 (HSP90), phytochrome-like ATPases and DNA mismatch repair proteins. Domains belonging to this superfamily are also referred to as GHKL (gyrase, heat-shock protein 90, histidine kinase, MutL) ATPase domains.',\n",
    "    'Curated_CD_Hierarchy': ['cd16915', 'cd16916', 'cd16917']\n",
    "}\n",
    "\n",
    "# TF-IDF on textual features\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([data['Short_Name'], data['Full_Name'], data['Description']])\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "# One-hot encoding of CDs\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# cd_encoder = OneHotEncoder(handle_unknown='ignore') \n",
    "# cd_matrix = cd_encoder.fit_transform(df[['Curated_CD_Hierarchy']])\n",
    "\n",
    "# # ... More advanced techniques for text embeddings and hierarchies\n",
    "\n",
    "# # Concatenate the results (assuming column alignment):\n",
    "# import numpy as np\n",
    "# feature_vector = np.concatenate([tfidf_matrix.toarray(), cd_matrix.toarray()], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a69fd-9988-4006-ac06-9228fda2dfee",
   "metadata": {},
   "source": [
    "### Try Hugging Face's tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "681950db-b0f4-41d0-87ef-b363584ecf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "[ 1.3406968e-03  6.6458713e-03  1.0026930e-02  9.0288976e-03\n",
      " -8.0215409e-03  6.3489946e-03 -5.5392282e-03 -7.7658350e-04\n",
      "  3.1031377e-04  6.5166638e-03  4.4480595e-03  4.5155901e-03\n",
      "  9.4607985e-03  4.7957644e-04 -6.0302089e-03 -6.2805046e-03\n",
      "  6.4116628e-03 -5.2691679e-03 -3.0001425e-03  3.9094579e-03\n",
      " -2.2884510e-03 -5.9413384e-03 -2.2987670e-03  1.1743887e-03\n",
      "  2.1965255e-03  6.0816943e-03 -5.1992629e-03  2.9741039e-03\n",
      "  7.2287843e-03  2.1823258e-03  5.4821507e-03 -4.8716874e-03\n",
      "  6.2371404e-03 -7.6808650e-03  3.4848591e-03 -9.2068017e-03\n",
      " -2.4689827e-03 -9.0560261e-03 -1.5378548e-03 -5.5116522e-03\n",
      " -3.9607170e-03  1.1452249e-03  2.7757741e-03 -1.5183004e-03\n",
      " -8.1028361e-03 -5.9405873e-03  7.6286914e-04 -3.9096614e-03\n",
      " -9.3703652e-03 -7.6693407e-04  6.6344640e-03  5.9785587e-03\n",
      " -9.9103302e-03  3.0971617e-03 -6.0076900e-03 -9.1484087e-03\n",
      "  1.9001009e-04 -3.9398295e-04 -7.0375144e-03 -6.1901347e-03\n",
      " -2.3787369e-03  7.1782465e-03 -7.5053875e-03  7.6678963e-03\n",
      " -4.9450807e-04  1.1764837e-03  9.5026717e-03  4.8119002e-03\n",
      " -3.6186299e-03  3.7773107e-03  3.4606862e-03  6.4068618e-03\n",
      "  9.3668175e-05 -4.4422797e-03  1.3860172e-03 -5.3739841e-03\n",
      "  1.3865514e-03  4.9402285e-03  5.1575885e-03  9.2202974e-03\n",
      " -7.4773403e-03 -5.3916951e-03  6.4624790e-03  1.4726896e-03\n",
      " -6.6383914e-03  9.3299913e-04  2.7396947e-03 -2.4787241e-03\n",
      " -4.8826355e-03  5.0208420e-03  9.7071789e-03 -7.2895023e-03\n",
      " -6.4675536e-05 -2.5089597e-03 -6.1749504e-03 -1.3383511e-03\n",
      " -5.2622627e-03  8.9854198e-03 -5.7702218e-03  3.6994098e-03] <class 'numpy.ndarray'>\n",
      "['Histidine', 'kinase', '-', 'like', 'ATPase', 'domain']\n",
      "[-3.53795790e-03  4.12180549e-03  7.73208264e-04  3.63083828e-04\n",
      " -9.15412481e-06 -4.16730882e-03  4.44247964e-03  3.66293964e-03\n",
      " -5.14998535e-03 -4.30240327e-03  1.02566274e-03 -4.78144042e-03\n",
      " -2.67436670e-03  5.09383176e-04 -8.18494785e-05 -1.99800336e-03\n",
      " -2.26145760e-03 -7.23668015e-04 -2.62431218e-03 -5.48301177e-03\n",
      "  7.89872720e-05  3.03868036e-03  6.69794863e-03 -2.08447256e-03\n",
      " -1.89786544e-04 -1.84490113e-03  4.44466064e-04 -2.84043581e-03\n",
      " -2.53457557e-04  9.15576335e-05  4.05585289e-03 -4.05578364e-03\n",
      "  3.75507959e-03 -1.19395178e-03 -5.11223695e-04  5.74597061e-04\n",
      "  3.21558093e-03  4.21547044e-03  4.44045775e-03 -1.81278350e-03\n",
      "  3.15058569e-03 -1.76023187e-03 -4.51829260e-03 -2.88992218e-04\n",
      " -1.53569801e-04  1.87679804e-03 -1.89384067e-04  6.52405007e-04\n",
      "  4.33259508e-04  2.39847490e-03 -1.55906384e-03 -4.90549263e-04\n",
      "  1.14975871e-03  1.28331541e-03  1.42380411e-04  1.32841795e-04\n",
      "  4.01623708e-03 -3.05566433e-03  1.07814092e-04 -4.62479851e-05\n",
      "  5.22659296e-04  2.15294581e-03 -4.07566084e-04 -5.30651443e-03\n",
      "  4.58167187e-04  3.60174876e-04  3.34846243e-03  1.20227155e-03\n",
      " -4.66586867e-03 -1.12976801e-04 -3.09391541e-03  5.38029184e-04\n",
      "  3.90223420e-03  3.01040969e-03 -1.53610725e-03 -1.05243606e-03\n",
      " -3.36284493e-03 -1.30197298e-03 -3.24550534e-03 -4.34596625e-04\n",
      "  1.66563802e-04 -1.34121832e-03  2.80109176e-03 -2.05625702e-04\n",
      "  1.14457951e-03 -3.05478228e-03  7.65583049e-04 -2.62922695e-03\n",
      " -9.45633743e-04  1.00881956e-03 -3.01861476e-04  6.79946019e-04\n",
      " -6.04492845e-04 -9.03582879e-04  4.74918936e-03  4.28622306e-03\n",
      "  7.80308677e-04 -2.15295634e-03 -1.90763302e-03  1.79134170e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mingzhang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download the punkt tokenizer models if you haven't already\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Customize pre-tokenization\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Train the tokenizer\n",
    "trainer = trainers.BpeTrainer(vocab_size=1000, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
    "tokenizer.train(files=[\"../data/cl00075\"], trainer=trainer)\n",
    "\n",
    "# Save the trained tokenizer\n",
    "tokenizer.save(\"my_tokenizer.json\")\n",
    "\n",
    "# Load the trained tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"my_tokenizer.json\")\n",
    "\n",
    "# Function to tokenize text using the loaded tokenizer\n",
    "def tokenize_sentence(text):\n",
    "    encoding = tokenizer.encode(text)\n",
    "    return encoding.tokens\n",
    "\n",
    "# Tokenize the corpus text file\n",
    "tokenized_corpus = []\n",
    "with open(\"../data/cl00075\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        tokenized_line = tokenize_sentence(line.strip())\n",
    "        tokenized_corpus.append(tokenized_line)\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained Word2Vec model\n",
    "model.save(\"word2vec_model.bin\")\n",
    "\n",
    "# print(tokenized_corpus)\n",
    "\n",
    "# # Finding similar words\n",
    "# similar_words = model.wv.most_similar('cl00075')\n",
    "# print(similar_words)\n",
    "\n",
    "# # Getting the vector representation of a word\n",
    "word_vector = model.wv['cl00075']\n",
    "print(word_vector, type(word_vector))\n",
    "\n",
    "# # Vector operations\n",
    "# result = model.wv.most_similar(positive=['cl00075', 'HATPase'], negative=[])\n",
    "# print(result)\n",
    "\n",
    "domain_name = 'Histidine kinase-like ATPase domain'\n",
    "enco = tokenizer.encode(domain_name)\n",
    "print(enco.tokens)\n",
    "\n",
    "def vectorize_sentence(sentence, word2vec_model):\n",
    "    tokens = tokenize_sentence(sentence)\n",
    "    sentence_embedding = np.zeros(word2vec_model.vector_size)  # Initialize sentence embedding vector\n",
    "    num_tokens = 0  # Keep track of the number of tokens with embeddings\n",
    "    for token in tokens:\n",
    "        if token in word2vec_model.wv:\n",
    "            sentence_embedding += word2vec_model.wv[token]\n",
    "            num_tokens += 1\n",
    "    if num_tokens > 0:\n",
    "        sentence_embedding /= num_tokens  # Average the embeddings\n",
    "    return sentence_embedding\n",
    "\n",
    "print(vectorize_sentence(domain_name, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e259a24-db87-408a-af72-7fd792475671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
